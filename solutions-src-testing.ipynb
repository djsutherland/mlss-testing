{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- SOLUTION CELL -->\n",
    "<h2 style=\"color: red\">This is the solutions file!</h2>\n",
    "\n",
    "<span style=\"color: red\">We recommend not looking at this file, which contains the solutions to all the exercises, until after the practical is over. Use [`testing.ipynb`](testing.ipynb) instead.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# This is put straight into the rendered nb so it renders untrusted...\n",
    "from IPython.display import display, Markdown\n",
    "with open('README-setup.md') as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\DeclareMathOperator{\\E}{\\mathbb E}\n",
    "\\DeclareMathOperator{\\Var}{Var}\n",
    "\\DeclareMathOperator{\\Tr}{Tr}\n",
    "\\DeclareMathOperator{\\MMD}{MMD}\n",
    "\\DeclareMathOperator{\\MMDhat}{\\widehat{MMD}}\n",
    "\\newcommand{\\PP}{\\mathbb{P}}\n",
    "\\newcommand{\\QQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\h}{\\mathcal H}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    if not os.path.exists('data/blobs.npz'):\n",
    "        !git clone https://github.com/dougalsutherland/mlss-testing\n",
    "        os.chdir('mlss-testing')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook')\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import tqdm  # if you're in JupyterLab/etc and this doesn't work well\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "\n",
    "import support\n",
    "from support import as_tensors, LazyKernel, maybe_squeeze, pil_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note\n",
    "\n",
    "Please ask for help if you get stuck! Whether it's with a code thing or something conceptual, your instructors are happy to help you try to work through things. Solutions are also available in [`solutions-testing.ipynb`](solutions-testing.ipynb), but you're probably better off not rushing straight to those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting out with two-sample testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# Generate the data...mean-shifted Gaussians\n",
    "N = 1000\n",
    "rs = np.random.RandomState(seed=0)\n",
    "X = rs.randn(N, 1).astype(np.float32)\n",
    "Y = rs.randn(N, 1).astype(np.float32) + 0.2\n",
    "np.savez('data/simple.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/simple.npz') as data:\n",
    "    X, Y = as_tensors(data['X'], data['Y'])\n",
    "    \n",
    "plt.hist(X.numpy(), alpha=.5, bins='auto', histtype='stepfilled')\n",
    "plt.hist(Y.numpy(), alpha=.5, bins='auto', histtype='stepfilled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: do $X$ and $Y$ come from the same distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by comparing the means of the samples.\n",
    "If the distributions are the same, their means are the same,\n",
    "so the expectation of this statistic would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_difference(X, Y, squared=False):\n",
    "    # make sure X and Y are 1d tensors:\n",
    "    # the data files have them as [n, 1] to go with Python convention\n",
    "    # (and for us to use later)\n",
    "    X, Y = [maybe_squeeze(t, 1) for t in as_tensors(X, Y)]\n",
    "    assert len(X.shape) == len(Y.shape) == 1\n",
    "    \n",
    "    # TODO: compute mean difference of X and Y in `result`\n",
    "    result = X.mean() - Y.mean()  # SOLUTION\n",
    "    \n",
    "    return (result * result) if squared else result\n",
    "\n",
    "mean_difference(X, Y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it's not zero...but of course we were never going to get *exactly* zero.\n",
    "\n",
    "Where do we draw the line?\n",
    "\n",
    "The classical statistical way to do this is to assume that $P_X = P_Y$ (the null hypothesis), and consider what the distribution of the test statistic would be in that case (the null distribution). If the number we observe would be very unlikely under the null distribution, then we \"reject the null\" and say that the two samples are different.\n",
    "\n",
    "One way to do this is called _permutation testing_. This is based on the observation that if $P_X = P_Y$, then we can shuffle the samples together, and that won't change the distribution of our shuffled $X'$ or $Y'$ (under the null hypothesis). We can then compute what our test statistic would be if we did this a bunch of times, and this will give us an estimate of what the true null distribution is. If we do a lot of permutation samples, and our test statistic landed above, say, 99% of them, then we can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def two_sample_permutation_test(\n",
    "    test_statistic, X, Y, num_permutations=500, progress=True\n",
    "):\n",
    "    X, Y = as_tensors(X, Y)\n",
    "    assert X.shape[1:] == Y.shape[1:]  # check same dimensionality\n",
    "    \n",
    "    orig_stat = test_statistic(X, Y)\n",
    "    \n",
    "    range_ = range(num_permutations)\n",
    "    if progress:\n",
    "        range_ = tqdm(range_)\n",
    "    \n",
    "    # concatenate samples together                    # SOLUTION\n",
    "    Z = torch.cat([X, Y], 0)                          # SOLUTION\n",
    "    n_X = X.shape[0]                                  # SOLUTION\n",
    "    \n",
    "    stats = []\n",
    "    for i in range_:\n",
    "        # TODO: compute test statistic on permuted samples, in `this_stat`\n",
    "        np.random.shuffle(Z.numpy())                  # SOLUTION\n",
    "        this_stat = test_statistic(Z[:n_X], Z[n_X:])  # SOLUTION\n",
    "        stats.append(this_stat)\n",
    "    return orig_stat, torch.stack(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a helper function I'll give you to visualize the results of the permutation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_null_samples(\n",
    "    statistic,        # the value of the actual test statistic\n",
    "    null_samples,     # an array of samples from the statistic's null distribution\n",
    "    ax=None,          # a matplotlib axis object; none for the default one\n",
    "    from_zero=False,  # indicate it's a one-sided test whose statistic is always >= 0\n",
    "    one_sided=False,  # indicate it's a one-sided test for high values of the statistic\n",
    "    alpha=1,          # transparency of the histogram for the null samples\n",
    "    level=0.05,       # level of the test, e.g. 0.05 for 5% false rejection rate\n",
    "):\n",
    "    null_samples = np.asarray(null_samples)\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.hist(\n",
    "        null_samples,\n",
    "        bins=\"auto\",\n",
    "        histtype=\"stepfilled\",\n",
    "        label=\"Permutation samples\",\n",
    "        alpha=alpha,\n",
    "    )\n",
    "\n",
    "    if from_zero:\n",
    "        lo = 0\n",
    "        hi = np.percentile(null_samples, 100 * (1 - level))\n",
    "    elif one_sided:\n",
    "        lo = np.min(null_samples) - 0.1 * (np.max(null_samples) - np.min(null_samples))\n",
    "        # should be -inf, but axvspan doesn't support that\n",
    "        hi = np.percentile(null_samples, 100 * (1 - level))\n",
    "    else:\n",
    "        lo = np.percentile(null_samples, 100 * level / 2)\n",
    "        hi = np.percentile(null_samples, 100 * (1 - level / 2))\n",
    "\n",
    "    ax.axvspan(lo, hi, fc=\"b\", alpha=0.25, label=f\"{1 - level:.1%} region\")\n",
    "\n",
    "    ax.axvline(x=statistic, c=\"r\", lw=2, label=\"Actual statistic\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "    ax.set_xlabel(\"Test statistic value\")\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "\n",
    "    if from_zero:\n",
    "        ax.set_xlim(0, ax.get_xlim()[1])\n",
    "    elif one_sided:\n",
    "        ax.set_xlim(lo, ax.get_xlim()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(mean_difference, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the test statistic, there might be many other ways to compute the null distribution.\n",
    "Not all of them work via sampling it directly like this \u2013 which can be pretty expensive, e.g. 500 times as expensive as the test in the first place....\n",
    "\n",
    "For example, for our mean difference test statistic, if we assume that $X$ and $Y$ are each Gaussian, the distribution of the statistic is also Gaussian \u2013\u00a0as we can see, at least roughly, in this plot. \n",
    "We might be even able to analytically work out the parameters of this distribution.\n",
    "\n",
    "If you haven't realized, this would be basically a [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test), where we assume that we know that both distributions have the same variance so we can distinguish them via solely looking at their mean. (In this particular case, that's true....)\n",
    "\n",
    "As it will be useful for kernel based test statistics later, we will also look at the distribution of a squared test statistic, in order to make it strictly positive, but still tend to zero if the two distributions are the same.\n",
    "\n",
    "Naturally, the average of squared Gaussian random variables has a chi-square distribution \u2013 something that can also be used for the kernel tests that we introduce later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_mean_diff = functools.partial(mean_difference, squared=True)\n",
    "# squared_mean_diff is a new function object that just calls mean_difference,\n",
    "# but defaults the `squared` argument to True\n",
    "\n",
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(squared_mean_diff, X, Y),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative distribution via explicit simulation\n",
    "\n",
    "The test statistic also has a distribution under the alternative hypothesis ($P_X \\neq P_Y$); our computed test statistic is only a single sample from this distribution.\n",
    "\n",
    "Unfortunately, it's not easy to look at this alternative distribution in practice,\n",
    "unless we have a way to generate more data.\n",
    "But let's take a look in a synthetic case where we know the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3); torch.manual_seed(0)\n",
    "\n",
    "# This is a case when the null hypothesis is false:\n",
    "N = 200\n",
    "sample_X = lambda: torch.randn(N, 1)\n",
    "sample_Y = lambda: torch.randn(N, 1) + 0.3\n",
    "\n",
    "# Do the normal testing thing, where we have one sample from each.\n",
    "X = sample_X()\n",
    "Y = sample_Y()\n",
    "\n",
    "# single sample from the alternative + null estimates\n",
    "stat, perm_samples = two_sample_permutation_test(squared_mean_diff, X, Y, num_permutations=500)\n",
    "\n",
    "statistics_alt = np.zeros(500)\n",
    "# TODO: fill statistics_alt with samples from the alternative distribution\n",
    "for i in tqdm(range(statistics_alt.shape[0])):                     # SOLUTION\n",
    "    statistics_alt[i] = squared_mean_diff(sample_X(), sample_Y())  # SOLUTION\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.hist(statistics_alt, alpha=0.5, bins='auto',\n",
    "        label='Alternative samples', histtype='stepfilled',\n",
    "        color=sns.color_palette()[1])\n",
    "plot_null_samples(stat, perm_samples, ax=ax, from_zero=True, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even though most of the alternative distribution is far larger than the null, we can be unlucky: there are a significant portion of datasets for which the test would not reject the null simply by chance.\n",
    "With this fixed seed, we got lucky, but a decent portion of the time we wouldn't reject the null, even though it's not true.\n",
    "\n",
    "**Optional:** Play with the sample size, or the amount of difference between $X$ and $Y$, and see how this impacts the position of the alternative distribution (and as such the test power)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** Actually generate more data from the null distribution, also. How well does the permutation distribution approximate the true null? How does this change with smaller / larger `N`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# gaussian vs laplace with same mean / variance\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(2)\n",
    "N = 300\n",
    "X = stats.norm.rvs(size=(N, 1)).astype(np.float32)\n",
    "Y = stats.laplace.rvs(size=(N, 1), scale=np.sqrt(.5)).astype(np.float32)\n",
    "\n",
    "np.savez('data/almost_simple.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/almost_simple.npz') as d:\n",
    "    X, Y = as_tensors(d['X'], d['Y'])\n",
    "\n",
    "plt.hist(X.numpy(), alpha=.5, bins='auto', histtype='stepfilled')\n",
    "plt.hist(Y.numpy(), alpha=.5, bins='auto', histtype='stepfilled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the mean won't cut it this time. Maybe the standard deviation?\n",
    "\n",
    "**Exercise:** Implement a two-sample test statistic based on both the mean and the standard deviation, e.g. $(\\mu_X - \\mu_Y)^2 + (\\sigma_X - \\sigma_Y)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_stat(X, Y):\n",
    "    X, Y = [maybe_squeeze(t, 1) for t in as_tensors(X, Y)]\n",
    "    assert len(X.shape) == len(Y.shape) == 1\n",
    "    \n",
    "    # TODO: return some statistic based on the mean and std dev\n",
    "    return (X.mean() - Y.mean()) ** 2 + (X.std() - Y.std()) ** 2    # SOLUTION\n",
    "\n",
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(mean_std_stat, X, Y),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least for the statistic I picked, this was comfortably inside the rejection region, even though visually the samples are quite different.\n",
    "\n",
    "Of course, it's possible we were just unlucky (as we saw could happen above) \u2013 but actually, the distributions I generated this data from have exactly the same means and standard deviations. Clearly, this test is never going to be able to tell such data apart, so it's time to move onto something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Mean Discrepancy\n",
    "\n",
    "As we saw in the lectures, the MMD can be thought of as either\n",
    "$$\n",
    "\\MMD(\\PP, \\QQ)\n",
    "= \\sup_{f \\in \\h : \\lVert f \\rVert_\\h \\le 1} \\E_{X \\sim \\PP}[f(X)] - \\E_{Y \\sim \\QQ}[ f(Y) ]\n",
    "$$\n",
    "or, more relevantly right now,\n",
    "$$\n",
    "\\MMD(\\PP, \\QQ)\n",
    "=\n",
    "\\lVert\n",
    "\\E_X[ \\varphi(X) ]\n",
    "- \\E_Y[ \\varphi(Y) ]\n",
    "\\rVert_\\h\n",
    "$$\n",
    "where $\\varphi : \\mathcal X \\to \\h$ is the feature map of the kernel $k(x, y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take $\\varphi(x) = x$, corresponding to a linear kernel, then our squared difference in means is exactly the squared MMD. But we can do a lot better by picking a different kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** does the difference in means + standard deviations you used above correspond to an MMD with some kernel? If so, what kernel? If not, is there a kernel MMD that can distinguish the same set of distributions as that can?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing kernels\n",
    "\n",
    "To help organize computing kernels, I've put some infrastructure in the `LazyKernel` class (in [`support.kernels`](support/kernels.py), if you want to look at it). It handles a bunch of gruntwork that you won't want to deal with later on. Here's an example of how to use it to implement a kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearKernel(LazyKernel):\n",
    "    def _compute(self, A, B):\n",
    "        return A @ B.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `_compute` method computes the kernel between two matrices of inputs, `A` and `B`, of shape `[n_A, dim]` and `[n_B, dim]`; it returns a kernel matrix of shape `[n_A, n_B]`. (Here, `.t()` is PyTorch syntax for a transpose, and `@` is the nifty Python 3.6+ syntax for matrix multiplication.)\n",
    "\n",
    "The `LazyKernel` base class lets us use this in various ways. First, to find the kernel from one set of points to another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = LinearKernel(X, Y)\n",
    "print(K)\n",
    "K.XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the X-to-X (`XX`) and Y-to-Y (`YY`) kernel matrices from the same object (which are the result of `_compute(X, X)` and `_compute(Y, Y)`). These aren't computed until you need them, but then they're cached after you use them the first time; this is why it's a `LazyKernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want the kernel matrix for a dataset to itself, you can just not pass the second argument. Then `K.XY` won't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearKernel(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass three arguments; then there'll be `XZ`, etc. You can also access them with e.g. `K[0, 2]`.\n",
    "\n",
    "Alternatively, you can pass `None`, which is a special value meaning \"use the first one.\" Then `XY` and so on will exist, but it knows to cache them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = LinearKernel(X, None, Y)\n",
    "print(K)\n",
    "K.YZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a slightly more complex kernel class, with some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialKernel(LazyKernel):\n",
    "    def __init__(self, X, *rest, degree=3, gamma=None, coef0=1):\n",
    "        super().__init__(X, *rest)\n",
    "        self.degree = degree\n",
    "        self.gamma = 1 / X.shape[1] if gamma is None else gamma\n",
    "        self.coef0 = coef0\n",
    "\n",
    "    def _compute(self, A, B):\n",
    "        XY = A @ B.t()\n",
    "        return (self.gamma * XY + self.coef0) ** self.degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also totally *don't need to do this* \u2013\u00a0you definitely won't notice the difference on this scale of data \u2013 but if you want to cache some computation for each dataset, there's a `_precompute` interface. You can return a list of cached information in `_precompute`, which then get passed to `_compute` as `_compute(A, *A_precomputed, B, *B_precomputed)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAndSquareKernel(LazyKernel):\n",
    "    def _precompute(self, A):\n",
    "        return [A * A]\n",
    "    \n",
    "    def _compute(self, A, A_squared, B, B_squared):\n",
    "        return A @ B.t() + A_squared @ B_squared.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're having trouble with the `_compute` matrix interface, you can also use a slower but simpler version where you only `_compute_one` for two entries at a time. This will definitely be noticeably slower, but probably still fine. (You don't need to implement `_compute_one` if you implement `_compute`; `_compute_one` doesn't support using `_precompute`.) Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAndSquareKernelSlower(LazyKernel):\n",
    "    def _compute_one(self, a, b):\n",
    "        return a @ b + (a * a) @ (b * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LinearAndSquareKernel(X, Y).XY - LinearAndSquareKernelSlower(X, Y).XY).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Remember from lecture that a *characteristic* kernel can distinguish *any* pair of distributions (with enough samples). None of the kernels we've implemented so far are characteristic. Your job is to implement one, the kernel that goes by names including Gaussian RBF, squared exponential, exponentiated quadratic, and probably more:\n",
    "\n",
    "$$k(X_i, X_j) = \\exp\\left( -\\frac{1}{2 \\sigma^2} \\lVert X_i - X_j \\rVert^2 \\right).$$\n",
    "\n",
    "It might be helpful to recall that\n",
    "$$\\lVert X_i - X_j \\rVert^2 = \\lVert X_i \\rVert^2 + \\lVert X_j \\rVert^2 - 2 X_i^T X_j;$$\n",
    "you should probably convert that into a matrix form to implement it.\n",
    "\n",
    "Alternatively, [`torch.pdist`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pdist) computes these distances, but it only gives you the upper triangle of the matrix so you'll have to turn it into a full symmetric matrix yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel(LazyKernel):\n",
    "    def __init__(self, *parts, sigma=1):\n",
    "        super().__init__(*parts)\n",
    "        self.sigma = sigma\n",
    "        self.const_diagonal = 1  # Says that k(x, x) = 1 for any x.\n",
    "                                 # Just a slight optimization; not really necessary.\n",
    "    \n",
    "    # TODO: implement _compute (maybe with _precompute) or _compute_one\n",
    "    def _precompute(self, A):                                            # SOLUTION\n",
    "        # Squared norms of each data point                               # SOLUTION\n",
    "        return [torch.einsum(\"ij,ij->i\", A, A)]                          # SOLUTION\n",
    "                                                                         # SOLUTION\n",
    "    def _compute(self, A, A_sqnorms, B, B_sqnorms):                      # SOLUTION\n",
    "        D2 = A_sqnorms[:, None] + B_sqnorms[None, :] - 2 * (A @ B.t())   # SOLUTION\n",
    "        return torch.exp(D2 / (-2 * self.sigma ** 2))                    # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your implementation against scikit-learn's implementation (but it doesn't work in PyTorch, so don't just use it directly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.random.lognormal()\n",
    "K = GaussianKernel(X, Y, sigma=sigma)\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "gamma = 1 / (2 * sigma**2)  # sklearn uses this parameterization\n",
    "assert np.allclose(K.XX.numpy(), rbf_kernel(X, gamma=gamma))\n",
    "assert np.allclose(K.XY.numpy(), rbf_kernel(X, Y, gamma=gamma))\n",
    "assert np.allclose(K.YY.numpy(), rbf_kernel(Y, gamma=gamma))\n",
    "\n",
    "del rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Matrix` classes\n",
    "\n",
    "Okay, one more handy thing about these `LazyKernel` classes that might be useful here. (You don't have to use them, but you can.)\n",
    "\n",
    "if you do `K.XY_m` (or `K.YY_m` or `K.matrix(0, 1)`, etc), then you get a special `support.kernels.Matrix` subclass. This implements \u2013\u00a0and caches \u2013\u00a0various operations you might need. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = GaussianKernel(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XY_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  '.join(m for m in dir(K.XY_m) if not m.startswith('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  '.join(m for m in dir(K.XX_m) if not m.startswith('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX_m.mean(), K.XX_m.offdiag_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the second half of [`support/kernels.py`](support/kernels.py) to see how they're implemented / what options there are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMD estimators\n",
    "\n",
    "Okay, enough admiring my beautiful code. Remember that we have\n",
    "\\begin{align}\n",
    "\\MMD^2(\\PP, \\QQ)\n",
    "  &= \\lVert \\E_X[ \\varphi(X) ] - \\E_Y[ \\varphi(Y) ] \\rVert_\\h^2\n",
    "\\\\&= \\langle\n",
    "        \\E_X[ \\varphi(X) ] - \\E_Y[ \\varphi(Y) ],\n",
    "        \\E_X[ \\varphi(X) ] - \\E_Y[ \\varphi(Y) ]\n",
    "     \\rangle_\\h\n",
    "\\\\&= \\langle \\E_X[ \\varphi(X) ], \\E_X[ \\varphi(X) \\rangle_\\h\n",
    "   + \\langle \\E_Y[ \\varphi(Y) ], \\E_Y[ \\varphi(Y) \\rangle_\\h\n",
    "   - 2 \\langle \\E_X[ \\varphi(X) ], \\E_Y[ \\varphi(Y) ] \\rangle_\\h\n",
    "\\\\&= \\E_{X, X', Y, Y'}\\left[\n",
    "     \\langle \\varphi(X), \\varphi(X') \\rangle_\\h\n",
    "   + \\langle \\varphi(Y), \\varphi(Y') \\rangle_\\h\n",
    "   - 2 \\langle \\varphi(X), \\varphi(Y) \\rangle_\\h\n",
    "   \\right]\n",
    "\\\\&= \\E_{X, X', Y, Y'}\\left[\n",
    "     k(X, X')\n",
    "   + k(Y, Y')\n",
    "   - 2 k(X, Y)\n",
    "   \\right]\n",
    ".\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a pretty natural idea for how to estimate the MMD. Well, three ideas of varying amounts of naturalness:\n",
    "\n",
    "The *biased* estimator (exactly the MMD between the empirical distributions) is, if we have $m$ samples from $X$ and $n$ from $Y$:\n",
    "$$\n",
    "\\MMDhat_b^2(X, Y)\n",
    "= \\frac{1}{m^2} \\sum_{i=1}^m \\sum_{j=1}^m k(X_i, X_j)\n",
    "+ \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n k(Y_i, Y_j)\n",
    "- \\frac{2}{m n} \\sum_{i=1}^m \\sum_{j=1}^n k(X_i, Y_j)\n",
    ".$$\n",
    "But this term has a bias due to the $k(X_i, X_i)$ and $k(Y_i, Y_i)$ terms. (You can tell that it's biased because if $X$ and $Y$ are each samples from the same distribution, the true MMD value is $0$, usually $\\MMDhat^2(X, Y) > 0$, and it's not possible to have $\\MMDhat^2(X, Y) < 0$ \u2013 thus $\\E \\MMDhat^2(X, Y) > 0$.)\n",
    "\n",
    "The *unbiased* estimator gets rid of these terms:\n",
    "$$\n",
    "\\MMDhat_u^2(X, Y)\n",
    "= \\frac{1}{m (m-1)} \\sum_{i \\ne j}^m k(X_i, X_j)\n",
    "+ \\frac{1}{n (n-1)} \\sum_{i \\ne j}^n k(Y_i, Y_j)\n",
    "- \\frac{2}{m n} \\sum_{i=1}^m \\sum_{j=1}^n k(X_i, Y_j)\n",
    ".$$\n",
    "This makes it unbiased, and in fact it's the minimum variance unbiased estimator.\n",
    "\n",
    "The $U$-statistic estimator only works when $n = m$, and also takes out the $k(X_i, Y_i)$ terms, which gives you a slightly worse estimator:\n",
    "$$\n",
    "\\MMDhat_U^2(X, Y)\n",
    "= \\frac{1}{m (m-1)} \\sum_{i \\ne j}^m \\left(\n",
    "    k(X_i, X_j) + k(Y_i, Y_j) - 2 k(X_i, Y_j)\n",
    "  \\right)\n",
    ".$$\n",
    "The advantage is that $U$-statistics have been studied pretty thoroughly by statisticians, so we know things about their variance and asymptotic distributions and whatnot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement at least one of these estimators, as functions of a `LazyKernel(X, Y)`. Using the `Matrix` helpers, each can be literally one line, but you can implement it however you feel like. You don't necessarily have to do all three; do at least one of them, but the others aren't too much extra work on top of the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd2_biased(K):\n",
    "    pass  # TODO: implement, using K.XX, K.XY, K.YY (or _m)\n",
    "    return K.XX_m.mean() + K.YY_m.mean() - 2 * K.XY_m.mean()  # SOLUTION\n",
    "\n",
    "def mmd2_unbiased(K):\n",
    "    pass  # TODO: implement\n",
    "    return K.XX_m.offdiag_mean() + K.YY_m.offdiag_mean() - 2 * K.XY_m.mean()  # SOLUTION\n",
    "\n",
    "def mmd2_u_stat(K):\n",
    "    assert K.ns[0] == K.ns[1]\n",
    "    # TODO: implement\n",
    "    return K.XX_m.offdiag_mean() + K.YY_m.offdiag_mean() - 2 * K.XY_m.offdiag_mean()  # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_biased(GaussianKernel(X, Y)), X, Y),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_unbiased(GaussianKernel(X, Y)), X, Y),\n",
    "    one_sided=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_u_stat(GaussianKernel(X, Y)), X, Y),\n",
    "    one_sided=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're all pretty much the same; when you're just doing permutation testing, the differences between them aren't super important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Run the test with a `LinearKernel`. Which of our estimators, if any, corresponds to the `mean_difference` statistic from before? (Do you understand why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_u_stat(LinearKernel(X, Y)), X, Y),\n",
    "    one_sided=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# Answer: it's mmd2_biased, as you can see e.g. by\n",
    "# expanding k(x, y) = x y in the equation for the sum.\n",
    "# Here's the numerical check:\n",
    "K = LinearKernel(X, Y)\n",
    "torch.stack([\n",
    "    mean_difference(X.squeeze(1), Y.squeeze(1), squared=True),\n",
    "    mmd2_biased(K),\n",
    "    mmd2_unbiased(K),\n",
    "    mmd2_u_stat(K),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** We're working on small datasets here so far, so this implementation of permutations is fine. But notice that we're recomputing the kernel matrix for each permutation, even though the matrices actually have all the same elements (just in jumbled-up order). Implement a faster way that doesn't involve this recomputation.\n",
    "\n",
    "Hint: this is easiest for the biased estimator, which you can write in a way amenable to [this kind of approach](https://pytorch.org/docs/stable/torch.html#torch.einsum). (The `LazyKernel.joint()` method, which concatenates all the kernel matrices together, might be useful.) Doing some algebra to work out a few slightly annoying correction terms, you can also do it similarly for the U-statistic estimator.\n",
    "\n",
    "If you're really, really careful, you can also make the unbiased estimator really fast (exploiting cache locality and things like that). We wrote about this in [this paper](https://arxiv.org/abs/1611.04488) (Section 3), and it's implemented in [Shogun](https://www.shogun-toolbox.org/). (Example usage in [here](https://nbviewer.jupyter.org/github/shogun-toolbox/shogun/blob/develop/doc/ipython-notebooks/statistical_testing/mmd_two_sample_testing.ipynb); there should be an API example [here](http://shogun.ml/examples/latest/examples/statistical_testing/quadratic_time_mmd.html) but the Shogun website is currently broken.)\n",
    "\n",
    "_Extra credit:_ empirically compare how our silly implementation here, your faster implementation, and the Shogun implementation scale as the dataset size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override this function if you implement a better one yourself,\n",
    "# and the stuff later will run faster....\n",
    "# This just establishes an API for passing in a kernel directly,\n",
    "# which is a little clumsy to implement, but it's not any faster.\n",
    "def mmd2_permutations_slow(K, use_biased=True, num_permutations=1000, progress=True):\n",
    "    # Some fiddling to be able to use the same kernel with \"new\" data.\n",
    "    from copy import copy\n",
    "    K_copy = copy(K)\n",
    "    def mmd2_with_K(X, Y):\n",
    "        K_copy.change_part(0, X)\n",
    "        K_copy.change_part(1, Y)\n",
    "        return (mmd2_biased if use_biased else mmd2_unbiased)(K_copy)\n",
    "    \n",
    "    return two_sample_permutation_test(\n",
    "        mmd2_with_K, K.X, K.Y,\n",
    "        num_permutations=num_permutations, progress=progress)\n",
    "\n",
    "mmd2_permutations = mmd2_permutations_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = GaussianKernel(X, Y, sigma=1)\n",
    "plot_null_samples(*mmd2_permutations_slow(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "\n",
    "# The easier one to understand here is the biased case:\n",
    "#   MMD_b^2(X, Y) = \\sum_{i,j} w_i Kjoint_{ij} w_j\n",
    "#   with w_i = 1/n_X if i is in X, -1/n_Y if it's in Y.\n",
    "# Then we just do that simultaneously for a bunch of ws.\n",
    "\n",
    "# The U-statistic case does basically the same thing, but then\n",
    "# adds some correction terms:\n",
    "#   MMD_U^2(X, Y) = (\n",
    "#      \\sum_ij K(X_i, X_j) + \\sum_ij K(Y_i, Y_j) - 2 \\sum_ij K(X_i, Y_j)\n",
    "#      - \\sum_i K(X_i, X_i) - \\sum_i K(Y_i, Y_i) - 2 \\sum_i K(X_i, Y_i)\n",
    "#   ) / (n_X * (n_X - 1))\n",
    "\n",
    "def mmd2_permutations(K, use_biased=True, permutations=1000, progress=None):\n",
    "    # progress is ignored\n",
    "    full_kernel = K.joint()\n",
    "    assert K.n_parts == 2\n",
    "    n_X, n_Y = K.ns\n",
    "    n = n_X + n_Y\n",
    "\n",
    "    if use_biased:\n",
    "        w_X = 1 / n_X\n",
    "        w_Y = -1 / n_Y\n",
    "    else:  # use U statistic\n",
    "        assert n_X == n_Y\n",
    "        w_X = 1\n",
    "        w_Y = -1\n",
    "\n",
    "    ws = torch.full((permutations + 1, n), w_Y,\n",
    "                    dtype=full_kernel.dtype, device=full_kernel.device)\n",
    "    ws[-1, :n_X] = w_X\n",
    "    for i in range(permutations):\n",
    "        ws[i, np.random.choice(n, n_X, replace=False)] = w_X\n",
    "\n",
    "    biased_ests = torch.einsum(\"pi,ij,pj->p\", ws, full_kernel, ws)\n",
    "    if use_biased:\n",
    "        ests = biased_ests\n",
    "    else:\n",
    "        # need to subtract \\sum_i k(X_i, X_i) + k(Y_i, Y_i) + 2 k(X_i, Y_i)\n",
    "        # first two are just trace, but last is harder:\n",
    "        is_X = ws > 0\n",
    "        X_inds = is_X.nonzero()[:, 1].view(permutations + 1, n_X)\n",
    "        Y_inds = (~is_X).nonzero()[:, 1].view(permutations + 1, n_Y)\n",
    "        del is_X, ws\n",
    "        cross_terms = full_kernel.take(Y_inds * n + X_inds).sum(1)\n",
    "        del X_inds, Y_inds\n",
    "        ests = (biased_ests - full_kernel.trace() + 2 * cross_terms) / (n_X * (n_X - 1))\n",
    "\n",
    "    est = ests[-1]\n",
    "    rest = ests[:-1]\n",
    "    return est, rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "plot_null_samples(*mmd2_permutations(K, use_biased=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative methods to estimate the null distribution\n",
    "\n",
    "There are many other way to get our hands on the distribution of the MMD test statistic under the null hypothesis.\n",
    "\n",
    "We know that asymptotically, the distribution is an infinite sum of Chi-square variables \u2013 but it's an infinite sum, and we don't even know what sum it is, so that doesn't help us too much.\n",
    "Techniques to approximate it include:\n",
    "\n",
    "* moment matching using a Gamma distribution: fast, but doesn't result in a consistent test.\n",
    "* a spectral approximation: using eigenvalues of the kernel matrix, we can estimate that infinite sum. Costly (cubic!) for large sample sets.\n",
    "* wild bootstrap: a technique used for correlated samples that is similar to permuting. (It's useful for e.g. testing an MCMC chain, though.)\n",
    "* linear time statistics: for those, one can often show that the null distribution is Gaussian, with a variance you can estimate in closed form.\n",
    "\n",
    "We won't cover any of these here, since they require some more details.\n",
    "The permutation test, while it can potentially be slow, is easy to understand and works for most applications (if implemented well).\n",
    "\n",
    "[This old Shogun notebook](https://nbviewer.jupyter.org/github/shogun-toolbox/shogun/blob/develop/doc/ipython-notebooks/statistical_testing/mmd_two_sample_testing.ipynb) contains many null approximation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD witness function in the RKHS\n",
    "\n",
    "One nice feature of the MMD is that we can see where the density functions are different by looking at the (empirical) MMD witness function.\n",
    "Remember that\n",
    "\\begin{align}\n",
    "\\MMD(\\PP, \\QQ)\n",
    "  &= \\sup_{f : \\lVert f \\rVert_\\h \\le 1} \\E_{X \\sim \\PP} f(X) - \\E_{Y \\sim \\QQ} f(Y)\n",
    "\\\\&= \\sup_{f : \\lVert f \\rVert_\\h \\le 1} \\langle f, \\E_{X \\sim \\PP}[\\varphi(X)] - \\E_{Y \\sim \\QQ}[\\varphi(Y)] \\rangle_\\h\n",
    "\\end{align}\n",
    "and so\n",
    "$$\n",
    "f^* \\propto \\E_{X \\sim \\PP}[\\varphi(X)] - \\E_{Y \\sim \\QQ}[\\varphi(Y)]\n",
    ",$$\n",
    "which means (using $f^*(t) = \\langle f^*, \\varphi(t) \\rangle_\\h$)\n",
    "$$\n",
    "f^*(t) \\propto \\E_{X \\sim \\PP} k(X, t) - \\E_{Y \\sim \\QQ} k(Y, t)\n",
    ".$$\n",
    "\n",
    "We can estimate $f^*$ with empirical averages.\n",
    "\n",
    "**Thought exercises:**\n",
    "- What's the proportionality constant hidden by $\\propto$?\n",
    "- Does the constant matter?\n",
    "- Which MMD estimator, if any, does directly estimating $f^*$ correspond to?\n",
    "\n",
    "The points where $\\lvert f^*(t) \\rvert$ are large are where the MMD test considers $\\PP$ and $\\QQ$ to be the \"most different.\"\n",
    "Let's define a grid and evaluate the witness function on it to see where that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd_witness(K, eval_pts):\n",
    "    assert K.n_parts == 2\n",
    "    K.append_part(eval_pts[:, None])  # now K.XZ compares X to eval_pts\n",
    "    \n",
    "    witness = 0\n",
    "    # TODO: estimate MMD witness function on grid_pts\n",
    "    witness = K.XZ.mean(0) - K.YZ.mean(0)  # SOLUTION\n",
    "    \n",
    "    K.drop_last_part()\n",
    "    witness = witness / mmd2_biased(K)  # to normalize; not necessary  # SOLUTION\n",
    "    return witness\n",
    "\n",
    "\n",
    "def plot_mmd_witness_1d(K, ax=None, grid_num=1000):\n",
    "    X, Y = [t.squeeze(1) for t in K.parts]\n",
    "    assert len(X.shape) == 1\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.hist(X.numpy(), alpha=.5, density=True, histtype='stepfilled', bins='auto')\n",
    "    ax.hist(Y.numpy(), alpha=.5, density=True, histtype='stepfilled', bins='auto')\n",
    "    ax.grid(False)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    lo, hi = ax2.get_xlim()\n",
    "    grid_pts = torch.linspace(lo, hi, grid_num)\n",
    "    ax2.plot(grid_pts.numpy(), mmd_witness(K, grid_pts).numpy(), color='k', lw=2)\n",
    "    ax2.set_xlim(lo, hi)\n",
    "    ax2.grid(False)\n",
    "    ax2.axhline(0, color='k', lw=0.5)\n",
    "    ax2.set_ylabel(\"Witness value\")\n",
    "    \n",
    "plot_mmd_witness_1d(GaussianKernel(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the witness function is positive where X as a higher density than Y, and negative vice versa.\n",
    "It is zero where both densities match.\n",
    "Intuitively, the RKHS norm of this function can only be zero if the densities match everywhere, and it grows as the densities differ on more and more points in their support.\n",
    "Of course, this kind of visualization only works in low dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel learning\n",
    "\n",
    "So far, we've been using `GaussianKernel` with its default bandwidth, 1. That's a pretty arbitrary choice; what if we tried some different values?\n",
    "\n",
    "**Thought exercise:** What do you expect the test results and witness functions to look like if we try larger or smaller sigmas (as the next block does)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sig in [.1, .001, 10]:\n",
    "    K = GaussianKernel(X, Y, sigma=sig)\n",
    "    fig, (a1, a2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "    plot_null_samples(*mmd2_permutations(K), ax=a1, from_zero=True)\n",
    "    plot_mmd_witness_1d(K, ax=a2)\n",
    "    fig.suptitle(f\"$\\sigma$ = {sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you intuitively understand what happened here (especially the witness plots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, bandwidth 1 is a totally arbitrary choice that depends on the scaling of the data.\n",
    "\n",
    "A [heuristic that usually works](https://twitter.com/dril/status/484722159462260736) is the \"median heuristic\": set $\\sigma$ to the median of the distance between data points.\n",
    "\n",
    "**Exercise:** compute the median distance between training data points. You can either compute this yourself, or use predefined functions, whichever you'd prefer. If it's too slow to compute, you can subsample the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_distance(Z):\n",
    "    # TODO: compute the median distance among the stacked samples Z\n",
    "    # If you want to be fancy, add options to optionally subset if Z is big.\n",
    "    median_dist = torch.median(torch.pdist(Z))  # SOLUTION\n",
    "    return median_dist\n",
    "\n",
    "med = median_distance(torch.cat([X, Y], 0))\n",
    "K = GaussianKernel(X, Y, sigma=med)\n",
    "\n",
    "fig, (a1, a2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "plot_null_samples(*mmd2_permutations(K), ax=a1, one_sided=True)\n",
    "plot_mmd_witness_1d(K, ax=a2)\n",
    "\n",
    "med.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the default 1 wasn't too far off, actually. The median heuristic worked great!\n",
    "\n",
    "But it doesn't always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dataset where the median heuristic doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def sample_blobs(n=500, ratio=0.01, rows=10, cols=10, sep=10, rs=None):\n",
    "    rs = check_random_state(rs)\n",
    "    # ratio is eigenvalue ratio\n",
    "    correlation = (ratio - 1) / (ratio + 1)\n",
    "\n",
    "    # generate within-blob variation\n",
    "    mu = np.zeros(2)\n",
    "    sigma = np.eye(2)\n",
    "    X = rs.multivariate_normal(mu, sigma, size=n)\n",
    "\n",
    "    corr_sigma = np.array([[1, correlation], [correlation, 1]])\n",
    "    Y = rs.multivariate_normal(mu, corr_sigma, size=n)\n",
    "\n",
    "    # assign to blobs\n",
    "    X[:, 0] += rs.randint(rows, size=n) * sep\n",
    "    X[:, 1] += rs.randint(cols, size=n) * sep\n",
    "    Y[:, 0] += rs.randint(rows, size=n) * sep\n",
    "    Y[:, 1] += rs.randint(cols, size=n) * sep\n",
    "\n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "X, Y = sample_blobs(rs=0)\n",
    "np.savez('data/blobs.npz', X=X, Y=Y)\n",
    "\n",
    "X, Y = sample_blobs(rs=1)\n",
    "np.savez('data/blobs2.npz', X=X, Y=Y)\n",
    "\n",
    "X, Y = sample_blobs(rs=2, rows=1, cols=1)\n",
    "np.savez('data/blobs_single.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "\n",
    "with np.load('data/blobs.npz') as d:\n",
    "    X, Y = as_tensors(d['X'], d['Y'])\n",
    "ax1.scatter(*X.t(), alpha=.5, label='X')\n",
    "ax1.scatter(*Y.t(), alpha=.5, label='Y')\n",
    "# ax1.legend()\n",
    "ax1.set_title(\"Blobs\")\n",
    "\n",
    "with np.load('data/blobs_single.npz') as d:\n",
    "    ax2.scatter(*d['X'].T, alpha=0.5, label='X')\n",
    "    ax2.scatter(*d['Y'].T, alpha=0.5, label='Y')\n",
    "ax2.legend()\n",
    "ax2.set_title(\"One blob (zoomed in)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two distributions are clearly very different. But the scale at which they're different is much smaller than the median distance, which looks mostly at global structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = median_distance(torch.cat([X, Y], 0))\n",
    "K = GaussianKernel(X, Y, sigma=med)\n",
    "\n",
    "plot_null_samples(*mmd2_permutations(K), from_zero=True)\n",
    "med.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a machine learner, your first instinct might be to try a bunch of different values and see which one works best.\n",
    "\n",
    "One question is what \"best\" means. Well, we could try out a bunch of values and see which kernel choice rejects the null most often.\n",
    "\n",
    "Note that we need to do this kernel fitting on *separate* data than conducting our final test. Otherwise we'd overfit, and be too likely to reject the null by post-hoc picking one that was big just due to chance. In real life, if `X` and `Y` were all we had, we'd have to split them up (and guess at how much to split them by...) into a \"training\" and a \"testing\" set.\n",
    "\n",
    "**Quick thought exercise:** What are the tradeoffs when picking a bigger training set versus a bigger testing set? When would one be a better choice? Can you try multiple values and see?\n",
    "\n",
    "**Answer:** If the kernel parameter is relatively unimportant, or easy to pick, then a bigger testing set will give you more of a chance to identify the difference in the data. But a bigger training set will give you more ability to pick the best kernel. Unfortunately, you can only try one split (unless you correct for multiple testing). <!-- SOLUTION -->\n",
    "\n",
    "Because I'm generous, I'm going to provide you with another set of samples to learn a kernel on. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/blobs2.npz') as d:\n",
    "    X_train, Y_train = as_tensors(d['X'], d['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional exercise:** implement cross-validation or a similar scheme to estimate kernel power. [`sklearn.model_selection`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) has some useful helpers if you want.\n",
    "\n",
    "You'll need to:\n",
    "- For each $\\sigma$ in a list of candidates:\n",
    "  - Split `X_train`, `Y_train` into subsets somehow (cross-validation or similar).\n",
    "  - For each split:\n",
    "    - Compute the $p$-value of a test based on the return value from `mmd2_permutations`.\n",
    "      (You might want to pass `progress=False` and only use an outer progress bar, for cleanliness.)\n",
    "    - Check whether the test rejected the null.\n",
    "  - Save the portion of times it rejected the null.\n",
    "- Compare (plot!) the power of each $\\sigma$.\n",
    "- Choose the best one, and use it to test on `X` and `Y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to give us an idea of which kernel parameter is better. Unfortunately, this approach is quite costly and noisy. In reality, we would also want to use more folds, repeat the cross-validation, and try out more kernel parameters (play with it and you will see). It's also tough to differentiate, so scaling to more complex kernel classes seems quite challenging.\n",
    "\n",
    "We could try one of the faster approximations for the null distribution mentioned above, for example the moment matching Gamma approach. This would speed up the procedure, but, the Gamma approximation has its limits in how accurate it is, especially for extreme values of the kernel.\n",
    "\n",
    "Instead, we'll do something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proxy criterion for kernel power\n",
    "\n",
    "It turns out that under the alternative $\\PP \\ne \\QQ$,\n",
    "$\\MMDhat_U^2$ is asymptotically normal,\n",
    "$$\n",
    "\\frac{\\MMDhat_U^2(X, Y) - \\MMD^2(\\PP, \\QQ)}{\\sqrt{\\Var_{X' \\sim \\PP^m, Y' \\sim \\QQ^m}\\left[ \\MMDhat_U^2(X', Y') \\right]}}\n",
    "\\stackrel{D}{\\to}\n",
    "\\mathcal N(0, 1)\n",
    ".$$\n",
    "Call that thing in the denominator,\n",
    "which is the variance of $\\MMDhat_U^2$ given samples of size $m$ from $\\PP$ and $\\QQ$,\n",
    "$V_m(\\PP, \\QQ)$.\n",
    "\n",
    "Under the null hypothesis $\\PP = \\QQ$,\n",
    "we have that $m \\MMDhat_U^2(X, Y)$ converges to some gross distribution (an infinite mixture of chi-squares).\n",
    "Let $c_\\alpha$ be the $(1-\\alpha)$th quantile of that distribution,\n",
    "the rejection threshold;\n",
    "we don't know that exactly,\n",
    "but we can estimate it with permutation testing as $\\hat c_\\alpha$.\n",
    "In this framework, note that $c_\\alpha$ depends on $\\PP$ and $\\QQ$ (and the kernel in the MMD)\n",
    "but it *doesn't* depend on $m$,\n",
    "and in the usual setting we have\n",
    "$\\hat c_\\alpha \\to c_\\alpha$.\n",
    "\n",
    "So, the power of our test \u2013\u00a0the probability of rejecting when $\\PP \\ne \\QQ$ \u2013 is just\n",
    "\n",
    "\\begin{align}\n",
    "     \\Pr\\left( m \\MMDhat_U^2(X, Y) > \\hat c_\\alpha \\right)\n",
    "  &= \\Pr\\left(\n",
    "       \\frac{\\MMDhat_U^2(X, Y) - \\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "     > \\frac{\\hat c_\\alpha / m - \\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "   \\right)\n",
    "\\\\&\\to 1 - \\Phi\\left(\n",
    "        \\frac{c_\\alpha / m - \\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "    \\right)\n",
    "\\\\&= \\Phi\\left(\n",
    "      \\frac{\\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "    - \\frac{c_\\alpha}{m \\sqrt{V_m(\\PP, \\QQ)}}\n",
    "    \\right)\n",
    "\\end{align}\n",
    "\n",
    "We can then maximize the power of the test by maximizing the thing inside the parentheses.\n",
    "\n",
    "Now, it turns out that for a fixed kernel, $V_m$ is $\\mathcal{O}(1 / m)$, and $\\MMD^2$ and $c_\\alpha$ are constants. So the first term is $\\mathcal{O}\\left(\\sqrt{m}\\right)$ and the second is $\\mathcal{O}\\left(1 / \\sqrt{m}\\right)$ \u2013\u00a0which means we should be able to just ignore the second term when $m$ is reasonably large.\n",
    "\n",
    "That means we need to choose $k$ to maximize $\\MMD^2(\\PP, \\QQ) / \\sqrt{V_m(\\PP, \\QQ)}$. We know how to estimate $\\MMD^2$. Turns out you can also estimate $V_m$ in the same quadratic time: see [this note](https://arxiv.org/abs/1906.02104).\n",
    "\n",
    "The note is 12 pages of dense algebra, so...I'll be here when you're done.\n",
    "\n",
    "(Don't actually read it.)\n",
    "\n",
    "**Bonus points:** Find a mistake in the algebra in the note. I checked it three times, but I wouldn't be at all surprised if there are still mistakes!\n",
    "\n",
    "**Big bonus points:** Figure out a way to simplify the derivation. Seriously, huge bonus points.\n",
    "\n",
    "Anyway, even the _expression_ for the estimator is too disgusting to write here. But I implemented it for you, in [`support.mmd2_u_stat_variance`](support/mmd.py). (*That's* what the `Matrix` subclasses are really for.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from support import mmd2_u_stat_variance\n",
    "\n",
    "help(mmd2_u_stat_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compare a fixed set of kernels, the different bandwidths on the blobs data, using this criterion. Does it give you the same choice as cross-validation? How many thousands of times faster is it?\n",
    "\n",
    "This is an unbiased estimator, so it might be negative, or zero! Make sure you don't divide by zero, or take the square root of a negative number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** We know how to estimate $c_\\alpha$ \u2013\u00a0with permutation testing \u2013 so we could optimize the more-complete criterion, too. Does that give you different answers? What about if $m$ is fairly small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent to optimize the power criterion\n",
    "\n",
    "What if we have more than one or two kernel parameters? What if we have dozens, or even (if we have really big datasets) thousands, or millions? Then trying out a grid of different combinations isn't going to work well. Instead, we can be good deep learners and do gradient descent.\n",
    "\n",
    "(This is why we did everything in PyTorch!)\n",
    "\n",
    "\n",
    "**Exercise:** Instead of doing a grid search, optimize $\\lambda$ with gradient descent.\n",
    "\n",
    "Does it always go to the same place, or can you get stuck in local minima?\n",
    "\n",
    "**Optional:** If you implemented a more efficient `mmd2_permutations`, and were a little careful about how you implemented it, you might even be able to differentiate your threshold estimate $\\hat c_\\alpha$ to optimize the full criterion. How does that compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN criticism with an ARD kernel\n",
    "\n",
    "You may have heard of Generative Adversarial Networks, GANs.\n",
    "Assume someone came to you with a new GAN, which generates some realistic looking images, which they say are indistinguishable from the images used to train the GAN.\n",
    "\n",
    "Actually, let's assume that person [said](https://arxiv.org/abs/1606.03498):\n",
    "> On MTurk, annotators were able to distinguish\n",
    "samples in 52.4% of cases (2000 votes total), where 50% would be obtained by random\n",
    "guessing. Similarly, researchers in our institution were not able to find any artifacts that would allow\n",
    "them to distinguish samples.\n",
    "\n",
    "Let's see what the kernel MMD thinks about this.\n",
    "We will use a slightly different kernel than before: an ARD (\"automatic relevance determination\") kernel.\n",
    "You can think of this kernel as scaling each of the $D$ input dimensions by a different parameter, and then uses a standard Gaussian kernel (unit bandwidth) on top of that.\n",
    "$$ k(x,y) = \\exp\\left(-\\frac12 \\sum_{d=1}^D \\left( s_d x_d - s_d y_d \\right)^2\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARDKernel(LazyKernel):\n",
    "    def __init__(self, *parts, scales=1):\n",
    "        super().__init__(*parts)\n",
    "        assert len(self.X.shape) == 2\n",
    "        d = self.X.shape[1]\n",
    "        \n",
    "        scales, = self.as_tensors(scales)\n",
    "        if scales.shape == ():\n",
    "            scales = scales.repeat(d)\n",
    "        else:\n",
    "            assert scales.shape == (d,)\n",
    "\n",
    "        self.scales = scales\n",
    "        self.const_diagonal = 1  # Says that k(x, x) = 1 for any x\n",
    "    \n",
    "    # TODO: implement _compute (maybe with _precompute)\n",
    "    def _precompute(self, A):                      # SOLUTION\n",
    "        s = A * self.scales[None, :]               # SOLUTION\n",
    "        return [s, torch.einsum(\"id,id->i\", s, s)] # SOLUTION\n",
    "    \n",
    "    def _compute(self, A, sc_A, sc_A_sqnorms, B, sc_B, sc_B_sqnorms):  # SOLUTION\n",
    "        D2 = (sc_A_sqnorms[:, None] + sc_B_sqnorms[None, :]            # SOLUTION\n",
    "              - 2 * (sc_A @ sc_B.t()))                                 # SOLUTION\n",
    "        return torch.exp(-0.5 * D2)                                    # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that ARDKernel at least agrees with GaussianKernel in the special case:\n",
    "Xtmp = torch.randn(3, 4)\n",
    "sigma = torch.exp(torch.randn(()))\n",
    "(ARDKernel(Xtmp, scales=1/sigma).XX\n",
    " - GaussianKernel(Xtmp, sigma=sigma).XX).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/gan-samples.npz') as d:\n",
    "    X, Y = [t.reshape(-1, 784).float() for t in as_tensors(d['X'], d['Y'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(\"X (true MNIST) samples:\")\n",
    "display(pil_grid(X[:40].reshape(-1, 1, 28, 28), nrow=20))\n",
    "display(\"Y (GAN) samples:\")\n",
    "display(pil_grid(Y[:40].reshape(-1, 1, 28, 28), nrow=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How does a median-heuristic Gaussian kernel perform on this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Split the data, and optimize an ARD kernel with gradient descent to compare them; then run the test on the other half of the data. Does it do better?\n",
    "\n",
    "Can you interpret the ARD weights `K.scales`?\n",
    "\n",
    "We can't do the 1d witness function plot, but can you think of a different way to figure out \"where\" the distributions are different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence testing\n",
    "\n",
    "So far, we've been asking whether $\\PP = \\QQ$ for two different distributions. Sometimes we'd rather do a related problem, *independence testing*. That is, we have paired data $\\{ (X_i, Y_i) \\}_{i=1}^n \\sim \\PP_{XY}^n$, and we want to know whether $X$ and $Y$ are independent or not.\n",
    "\n",
    "By definition, this is the same thing as asking whether $\\PP_{XY} = \\PP_X \\times \\PP_Y$, the product of the marginal distributions. But we know how to do that: if we use a characteristic kernel on the joint space, they're independent if and only if $\\MMD(\\PP_{XY}, \\PP_{X} \\times \\PP_Y) = 0$.\n",
    "The quantity $\\MMD(\\PP_{XY}, \\PP_{X} \\times \\PP_Y)^2$  is also called the HSIC (\"Hilbert-Schmidt Independence Criterion\"). \n",
    "\n",
    "Usually we choose a kernel $k$ for $X$ and a kernel $\\ell$ for $L$, and use the _product kernel_ on the joint space:\n",
    "$$\n",
    "  (k \\times \\ell)( (x, y), (x', y') )\n",
    "  = k(x, x') \\ell(y, y')\n",
    ".$$\n",
    "It turns out you can estimate it like this:\n",
    "$$\n",
    "\\frac{1}{n^2} \\Tr\\left( H K H L \\right)\n",
    ",$$\n",
    "where $K$ is the kernel matrix on the $X$ samples,\n",
    "$L$ is the kernel matrix on the $L$ samples,\n",
    "and $H = I - \\frac1n \\mathbf{1} \\mathbf{1}^T$ is the _centering matrix_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the HSIC estimator. You can do it literally as written here, with an H matrix and taking matrix products and so on. Or, if you want a more efficient implementation \u2013\u00a0$\\mathcal{O}(n^2)$ instead of $\\mathcal{O}(n^3)$ \u2013 you can notice a few things:\n",
    "\n",
    "- If you expand out what $H K H$ does, it can be done in quadratic time without actually constructing $H$.\n",
    "- $\\Tr(A B) = \\sum_i ( A B )_{ii} = \\sum_i \\sum_j A_{ij} B_{ji}$, which you can easily implement in quadratic time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_kernel_matrices(K, L):\n",
    "    if isinstance(K, LazyKernel):\n",
    "        K = K.XX\n",
    "    if isinstance(L, LazyKernel):\n",
    "        L = L.XX\n",
    "    K, L = as_tensors(K, L)\n",
    "    assert len(K.shape) == len(L.shape) == 2\n",
    "    assert K.shape[0] == K.shape[1] == L.shape[0] == L.shape[1]\n",
    "    return K, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_est(K, L):\n",
    "    K, L = _as_kernel_matrices(K, L)\n",
    "    \n",
    "    # TODO: implement the HSIC estimator\n",
    "    \n",
    "    if False:  # the slower way:                          # SOLUTION\n",
    "        n = K.shape[0]                                    # SOLUTION\n",
    "        H = torch.eye(n, dtype=K.dtype, device=K.device)  # SOLUTION\n",
    "        hsic = (H @ K @ H @ L).trace() / (n * n)          # SOLUTION\n",
    "    else:                                                 # SOLUTION\n",
    "        # (I - 1/n 1 1^T) K (I - 1/n 1 1^T)               # SOLUTION\n",
    "        #   = K - 1/n 1 (1^T K) - 1/n (K 1) 1^T           # SOLUTION\n",
    "        #       + 1/n^2 1 (1^T K 1) 1^T                   # SOLUTION\n",
    "        row_means = K.mean(0, keepdim=True)               # SOLUTION\n",
    "        col_means = row_means.t()  # K is symmetric       # SOLUTION\n",
    "        grand_mean = row_means.mean(1, keepdim=True)      # SOLUTION\n",
    "        K_cent = K - row_means - col_means + grand_mean   # SOLUTION\n",
    "        L_transp = L  # L is symmetric too                # SOLUTION\n",
    "        hsic = (K_cent * L_transp).mean()                 # SOLUTION\n",
    "    return hsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a test, we'll also need to do permutations. Note that permutations are slightly different from before: we'll need to permute $K$ and $L$ *separately*, so that $\\{(x_i, y_i)\\}$ becomes $\\{(x_i, y_j)\\}$, and any possible dependence between them is broken.\n",
    "\n",
    "Here's some framework for computing permuations for the HSIC estimator. If you want to do it a faster way, go ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_permutations(K, L, num_permutations=1000, progress=True):\n",
    "    K, L = _as_kernel_matrices(K, L)\n",
    "\n",
    "    est = hsic_est(K, L)\n",
    "\n",
    "    range_ = range(num_permutations)\n",
    "    if progress:\n",
    "        range_ = tqdm(range_)\n",
    "    \n",
    "    n = K.shape[0]       # SOLUTION\n",
    "    stats = []\n",
    "    for i in range_:\n",
    "        pass # TODO: permute the entries of K and L and call hsic_est\n",
    "        oK = torch.randperm(n)            # SOLUTION\n",
    "        Kp = K[oK[:, None], oK[None, :]]  # SOLUTION\n",
    "        oL = torch.randperm(n)            # SOLUTION\n",
    "        Lp = L[oL[:, None], oL[None, :]]  # SOLUTION\n",
    "        stats.append(hsic_est(Kp, Lp))    # SOLUTION\n",
    "        # Note: we really only need to shufle one of them.              # SOLUTION\n",
    "        # And centering isn't affected by the order,                    # SOLUTION\n",
    "        # so it'd be a little faster to center beforehand and keep it.  # SOLUTION\n",
    "        \n",
    "    return est, torch.stack(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on some simple data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "def sample_hsic(n, angle, sigma=0.2, offset=1):\n",
    "    n4 = int(n/4)\n",
    "    N = np.random.randn(n4, 2)*sigma\n",
    "    S = np.random.randn(n4, 2)*sigma\n",
    "    E = np.random.randn(n4, 2)*sigma\n",
    "    W = np.random.randn(n4, 2)*sigma\n",
    "    \n",
    "    N[:,1] += offset\n",
    "    S[:,1] -= offset\n",
    "    W[:,0] -= offset\n",
    "    E[:,0] += offset\n",
    "    \n",
    "    R = np.array([[np.cos(angle), -np.sin(angle)],[np.sin(angle), np.cos(angle)]])\n",
    "    A = R.dot(np.vstack((N,S,W,E)).T).T\n",
    "    \n",
    "    A = A.astype(np.float32)\n",
    "    return A[:, 0], A[:, 1]\n",
    "\n",
    "N = 200\n",
    "np.random.seed(0)\n",
    "X, Y = sample_hsic(n=N, angle=np.pi/12)\n",
    "np.savez(\"data/hsic.npz\", X=X[:, None], Y=Y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/hsic.npz') as d:\n",
    "    X, Y = as_tensors(d['X'], d['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there is some very simple dependence going on.\n",
    "The first thing a statistician would do is of course to check for correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation between X and Y:\", np.corrcoef(X.squeeze(), Y.squeeze())[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is very low.\n",
    "A kernel test for independence is more powerful, even with a bandwidth parameter chosen by the median heuristic.\n",
    "\n",
    "**Exercise:** Implement and visualize this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "\n",
    "K = GaussianKernel(X, Y, sigma=median_distance(X))\n",
    "L = GaussianKernel(X, Y, sigma=median_distance(Y))\n",
    "plot_null_samples(*hsic_permutations(K, L), from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty resounding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- SOLUTION CELL -->\n",
    "## European parliament documents translations and string kernels\n",
    "\n",
    "We will now do a slightly more elaborate test, that involves some very mild NLP:\n",
    "we'll analyze dependence between documents.\n",
    "We will use transcripts of the Canadian parliament's house debates, downloaded from [here](https://www.isi.edu/natural-language/download/hansard/).\n",
    "Those consist of pairs of French and English transcripts. Here's the end of two corresponding documents:\n",
    "\n",
    "> d until tomorrow at 2 p.m., pursuant to Standing Order 24(1).  \n",
    "> (The House adjourned at 6.41 p.m.) \n",
    "\n",
    "> main, \u00e0 10 heures, conform\u00e9ment \u00e0 l'article 24(1) du R\u00e8glement.  \n",
    "> (La s\u00e9ance est lev\u00e9e \u00e0 18 h 20.) \n",
    "\n",
    "Our question here is whether we can detect this supposedly strong dependence structure using the kernel HSIC.\n",
    "Note that this approach does not rely on attempting to translate the documents, but rather on comparing within-document structure. \n",
    "HSIC compares the self-similarity within the English documents with self-similarity of the French ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to construct a string kernel, a  \"bag of words\" kernel between documents $s$ and $t$,\n",
    "$$\n",
    "k(s,t) = \\frac{1}{|\\mathcal{W}|} \\phi(s)^\\top \\phi(t)\n",
    "$$\n",
    "where $\\mathcal{W}$ consists of all words on all considered documents, and each element of $\\phi(x)\\in\\mathbb{N}^{|\\mathcal{W}|}$ contains the number of times that a particular word $w\\in\\mathcal{W}$ appears in $x$.\n",
    "\n",
    "Naturally, the kernel value will be larger if a word appears in both documents many times.\n",
    "\n",
    "Efficient implementations of string kernels are rare (Shogun has many!), and often based on low-level dynamic programming concepts.\n",
    "Instead, we will here explicitly embed the documents into a feature space and compute (gram) matrix of inner products manually. \n",
    "Note that most string kernel implementations are much more efficient in computing the kernel since they don't do the feature space mapping explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords, from nltk.corpus.stopwords\n",
    "with open('data/stopwords-english.txt') as f:\n",
    "    en_stop = {x.strip() for x in f}\n",
    "with open('data/stopwords-french.txt') as f:\n",
    "    fr_stop = {x.strip() for x in f}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things faster, we're going to use the \"hashing trick\" to featurize this documents as bags of words \u2013\u00a0that is, each document is a count of how many times each unique word showed up. Rather than keeping track of a vocabulary, we use a hash function to map each word to a (probably) unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tarfile\n",
    "from scipy import sparse\n",
    "\n",
    "en_feats = {}\n",
    "fr_feats = {}\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "en_vec = HashingVectorizer(\n",
    "    analyzer=\"word\", decode_error='replace', stop_words=en_stop,\n",
    ")\n",
    "fr_vec = HashingVectorizer(\n",
    "    analyzer=\"word\", decode_error='replace', stop_words=fr_stop,\n",
    ")\n",
    "\n",
    "with tarfile.open('data/transcripts.tar.bz2') as tar:\n",
    "    for info in tqdm(tar, total=627):\n",
    "        fname = info.name\n",
    "        is_french = fname.endswith(\"f\")\n",
    "        is_english = fname.endswith(\"e\")\n",
    "        if not (is_english or is_french):\n",
    "            continue\n",
    "        \n",
    "        with tar.extractfile(info) as f:\n",
    "            if is_english:\n",
    "                en_feats[fname] = en_vec.transform([f.read()])\n",
    "            else:\n",
    "                fr_feats[fname] = fr_vec.transform([f.read()])\n",
    "\n",
    "assert len(en_feats) == len(fr_feats)\n",
    "names = {k[:-1] for k in en_feats}\n",
    "\n",
    "X = sparse.vstack([en_feats[k + 'e'] for k in names])\n",
    "Y = sparse.vstack([fr_feats[k + 'f'] for k in names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features are enormous (and enormously sparse) matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Construct linear kernel matrices for the French and English data.\n",
    "\n",
    "Now, our PyTorch kernel infrastructure isn't going to love trying to construct a matrix with 1 million columns. We're only going to use a linear kernel, though, so we can just compute the kernel ourselves. (You can convert from these sparse matrix classes to numpy arrays with `toarray()`, but do it *after* computing the kernel matrix to avoid trying to allocate an enormous dense array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute kernels\n",
    "K_XX = (\n",
    "    # ...\n",
    "    X @ X.T   # SOLUTION\n",
    ").toarray().astype(np.float32)\n",
    "K_YY = (\n",
    "    # ...\n",
    "    Y @ Y.T   # SOLUTION\n",
    ").toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Run the HSIC test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "plot_null_samples(*hsic_permutations(K_XX, K_YY), from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the English and French documents are not independent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
