{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- SOLUTION CELL -->\n",
    "<h2 style=\"color: red\">This is the solutions file!</h2>\n",
    "\n",
    "<span style=\"color: red\">We recommend not looking at this file, which contains the solutions to all the exercises, until after the practical is over. Use [`testing.ipynb`](testing.ipynb) instead.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# This is put straight into the rendered nb so it renders untrusted...\n",
    "from IPython.display import display, Markdown\n",
    "with open('README-setup.md') as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\DeclareMathOperator{\\E}{\\mathbb E}\n",
    "\\DeclareMathOperator{\\Var}{Var}\n",
    "\\DeclareMathOperator{\\Tr}{Tr}\n",
    "\\DeclareMathOperator{\\MMD}{MMD}\n",
    "\\DeclareMathOperator{\\MMDhat}{\\widehat{MMD}}\n",
    "\\newcommand{\\PP}{\\mathbb{P}}\n",
    "\\newcommand{\\QQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\h}{\\mathcal H}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    if not os.path.exists('data/blobs.npz'):\n",
    "        !git clone https://github.com/dougalsutherland/mlss-testing\n",
    "        os.chdir('mlss-testing')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook')\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import tqdm  # if you're in JupyterLab/etc and tqdm_notebook doesn't work well\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "\n",
    "import support\n",
    "from support import as_tensors, LazyKernel, maybe_squeeze, pil_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note\n",
    "\n",
    "Please ask for help if you get stuck! Whether it's with a code thing or something conceptual, your instructors are happy to help you try to work through things. Solutions are also available in [`solutions-testing.ipynb`](solutions-testing.ipynb), but you're probably better off not rushing straight to those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting out with two-sample testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# Generate the data...mean-shifted Gaussians\n",
    "N = 1000\n",
    "rs = np.random.RandomState(seed=0)\n",
    "X = rs.randn(N, 1).astype(np.float32)\n",
    "Y = rs.randn(N, 1).astype(np.float32) + 0.2\n",
    "np.savez('data/simple.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/simple.npz') as data:\n",
    "    X, Y = as_tensors(data['X'], data['Y'])\n",
    "    \n",
    "plt.hist(X.numpy(), alpha=.5, bins='auto', histtype='stepfilled')\n",
    "plt.hist(Y.numpy(), alpha=.5, bins='auto', histtype='stepfilled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: do $X$ and $Y$ come from the same distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by comparing the means of the samples.\n",
    "If the distributions are the same, their means are the same,\n",
    "so the expectation of this statistic would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_difference(X, Y, squared=False):\n",
    "    # make sure X and Y are 1d tensors:\n",
    "    # the data files have them as [n, 1] to go with Python convention\n",
    "    # (and for us to use later)\n",
    "    X, Y = [maybe_squeeze(t, 1) for t in as_tensors(X, Y)]\n",
    "    assert len(X.shape) == len(Y.shape) == 1\n",
    "    \n",
    "    # TODO: compute mean difference of X and Y in `result`\n",
    "    result = X.mean() - Y.mean()  # SOLUTION\n",
    "    \n",
    "    return (result * result) if squared else result\n",
    "\n",
    "mean_difference(X, Y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it's not zero...but of course we were never going to get *exactly* zero.\n",
    "\n",
    "Where do we draw the line?\n",
    "\n",
    "The classical statistical way to do this is to assume that $P_X = P_Y$ (the null hypothesis), and consider what the distribution of the test statistic would be in that case (the null distribution). If the number we observe would be very unlikely under the null distribution, then we \"reject the null\" and say that the two samples are different.\n",
    "\n",
    "One way to do this is called _permutation testing_. This is based on the observation that if $P_X = P_Y$, then we can shuffle the samples together, and that won't change the distribution of our shuffled $X'$ or $Y'$ (under the null hypothesis). We can then compute what our test statistic would be if we did this a bunch of times, and this will give us an estimate of what the true null distribution is. If we do a lot of permutation samples, and our test statistic landed above, say, 99% of them, then we can reject the null hypothesis.\n",
    "\n",
    "**Exercise:** fill in the following method for implementing two-sample tests. Here `test_statistic` is a variable representing a function, like `mean_difference`, that will compute some statistic given two inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def two_sample_permutation_test(\n",
    "    test_statistic, X, Y, num_permutations=500, progress=True\n",
    "):\n",
    "    X, Y = as_tensors(X, Y)\n",
    "    assert X.shape[1:] == Y.shape[1:]  # check same dimensionality\n",
    "    \n",
    "    orig_stat = test_statistic(X, Y)\n",
    "    \n",
    "    range_ = range(num_permutations)\n",
    "    if progress:\n",
    "        range_ = tqdm(range_)\n",
    "    \n",
    "    # concatenate samples together                    # SOLUTION\n",
    "    Z = torch.cat([X, Y], 0)                          # SOLUTION\n",
    "    n_X = X.shape[0]                                  # SOLUTION\n",
    "    \n",
    "    stats = []\n",
    "    for i in range_:\n",
    "        # TODO: compute test statistic on permuted samples, in `this_stat`\n",
    "        np.random.shuffle(Z.numpy())                  # SOLUTION\n",
    "        this_stat = test_statistic(Z[:n_X], Z[n_X:])  # SOLUTION\n",
    "        stats.append(this_stat)\n",
    "    \n",
    "    return orig_stat, torch.stack(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a helper function I'll give you to visualize the results of the permutation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_null_samples(\n",
    "    statistic,        # the value of the actual test statistic\n",
    "    null_samples,     # an array of samples from the statistic's null distribution\n",
    "    ax=None,          # a matplotlib axis object; none for the default one\n",
    "    from_zero=False,  # indicate it's a one-sided test whose statistic is always >= 0\n",
    "    one_sided=True,   # indicate it's a one-sided test for high values of the statistic\n",
    "    alpha=1,          # transparency of the histogram for the null samples\n",
    "    level=0.05,       # level of the test, e.g. 0.05 for 5% false rejection rate\n",
    "):\n",
    "    null_samples = np.asarray(null_samples)\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.hist(\n",
    "        null_samples,\n",
    "        bins=\"auto\",\n",
    "        histtype=\"stepfilled\",\n",
    "        label=\"Permutation samples\",\n",
    "        alpha=alpha,\n",
    "        density=True,\n",
    "    )\n",
    "\n",
    "    if from_zero:\n",
    "        lo = 0\n",
    "        hi = np.percentile(null_samples, 100 * (1 - level))\n",
    "    elif one_sided:\n",
    "        lo = np.min(null_samples) - 0.02 * (np.max(null_samples) - np.min(null_samples))\n",
    "        # should be -inf, but axvspan doesn't support that\n",
    "        hi = np.percentile(null_samples, 100 * (1 - level))\n",
    "    else:\n",
    "        lo = np.percentile(null_samples, 100 * level / 2)\n",
    "        hi = np.percentile(null_samples, 100 * (1 - level / 2))\n",
    "\n",
    "    ax.axvspan(lo, hi, fc=\"b\", alpha=0.25, label=f\"{1 - level:.1%} region\")\n",
    "\n",
    "    ax.axvline(x=statistic, c=\"r\", lw=2, label=\"Actual statistic\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "    ax.set_xlabel(\"Test statistic value\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "\n",
    "    if from_zero:\n",
    "        ax.set_xlim(0, ax.get_xlim()[1])\n",
    "    elif one_sided:\n",
    "        ax.set_xlim(lo, ax.get_xlim()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(mean_difference, X, Y),\n",
    "    one_sided=False)\n",
    "\n",
    "# The * here is a bit of Python helper syntax. If you haven't seen it before,\n",
    "# fn(*it) calls fn like fn(it[0], it[1], ...) for the length of it,\n",
    "# so here it's like doing\n",
    "#   stat, nulls = two_sample_permutation_test(...)\n",
    "#   plot_null_samples(stat, nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the test statistic, there might be many other ways to compute the null distribution.\n",
    "Not all of them work via sampling it directly like this \u2013 which can be pretty expensive, e.g. 500 times as expensive as the test in the first place....\n",
    "\n",
    "For example, for our mean difference test statistic, if we assume that $X$ and $Y$ are each Gaussian, the distribution of the statistic is also Gaussian \u2013\u00a0as we can see, at least roughly, in this plot. \n",
    "We might be even able to analytically work out the parameters of this distribution.\n",
    "\n",
    "If you haven't realized, this would be basically a [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test), where we assume that we know that both distributions have the same variance so we can distinguish them via solely looking at their mean. (In this particular case, that's true....)\n",
    "\n",
    "The advantage of permutation tests, though, is that you don't have to make strong parametric assumptions like this.\n",
    "\n",
    "As it will be useful for kernel based test statistics later, we will also look at the distribution of a squared test statistic, in order to make it strictly positive, but still tend to zero if the two distributions are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_mean_diff = functools.partial(mean_difference, squared=True)\n",
    "# squared_mean_diff is a new function object that just calls mean_difference,\n",
    "# but defaults the `squared` argument to True\n",
    "\n",
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(squared_mean_diff, X, Y),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative distribution via explicit simulation\n",
    "\n",
    "The test statistic also has a distribution under the alternative hypothesis ($P_X \\neq P_Y$); our computed test statistic is only a single sample from this distribution.\n",
    "\n",
    "Unfortunately, it's not easy to look at this alternative distribution in practice,\n",
    "unless we have a way to generate more data.\n",
    "But let's take a look in a synthetic case where we know the true distribution.\n",
    "\n",
    "**Exercise:** sample from the alternative to make the comparison plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0); torch.manual_seed(5)\n",
    "\n",
    "# This is a case when the null hypothesis is false:\n",
    "N = 200\n",
    "sample_X = lambda: torch.randn(N, 1)\n",
    "sample_Y = lambda: torch.randn(N, 1) + 0.3\n",
    "\n",
    "# Do the normal testing thing, where we have one sample from each.\n",
    "X = sample_X()\n",
    "Y = sample_Y()\n",
    "\n",
    "# single sample from the alternative + null estimates\n",
    "stat, perm_samples = two_sample_permutation_test(squared_mean_diff, X, Y, num_permutations=500)\n",
    "\n",
    "statistics_alt = np.zeros(500)\n",
    "# TODO: fill statistics_alt with samples from the alternative distribution\n",
    "for i in tqdm(range(statistics_alt.shape[0])):                     # SOLUTION\n",
    "    statistics_alt[i] = squared_mean_diff(sample_X(), sample_Y())  # SOLUTION\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.hist(statistics_alt, alpha=0.5, bins='auto', density=True,\n",
    "        label='Alternative samples', histtype='stepfilled',\n",
    "        color=sns.color_palette()[1])\n",
    "plot_null_samples(stat, perm_samples, ax=ax, from_zero=True, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even though most of the alternative distribution is far larger than the null, we can be unlucky: there are a significant portion of datasets for which the test would not reject the null simply by chance.\n",
    "This particular seed was one of the unlucky ones: most of the time we'd reject, but this time we didn't. The probability with which the test rejects when the alternative is false is called the *power*.\n",
    "\n",
    "**Optional:** Play with the sample size, or the amount of difference between $X$ and $Y$, and see how this impacts the position of the alternative distribution (and as such the test power)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** Actually generate more data from the null distribution, also. How well does the permutation distribution approximate the true null? How does this change with smaller / larger `N`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# gaussian vs laplace with same mean / variance\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(2)\n",
    "N = 300\n",
    "X = stats.norm.rvs(size=(N, 1)).astype(np.float32)\n",
    "Y = stats.laplace.rvs(size=(N, 1), scale=np.sqrt(.5)).astype(np.float32)\n",
    "\n",
    "np.savez('data/almost_simple.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/almost_simple.npz') as d:\n",
    "    X, Y = as_tensors(d['X'], d['Y'])\n",
    "\n",
    "plt.hist(X.numpy(), alpha=.5, bins='auto', histtype='stepfilled')\n",
    "plt.hist(Y.numpy(), alpha=.5, bins='auto', histtype='stepfilled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the mean won't cut it this time. Maybe the standard deviation?\n",
    "\n",
    "**Exercise:** Implement a two-sample test statistic based on both the mean and the standard deviation, e.g. $(\\mu_X - \\mu_Y)^2 + (\\sigma_X - \\sigma_Y)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_stat(X, Y):\n",
    "    # make data 1d\n",
    "    X, Y = [maybe_squeeze(t, 1) for t in as_tensors(X, Y)]\n",
    "    assert len(X.shape) == len(Y.shape) == 1\n",
    "    \n",
    "    # TODO: return some statistic based on the mean and std dev\n",
    "    return (X.mean() - Y.mean()) ** 2 + (X.std() - Y.std()) ** 2    # SOLUTION\n",
    "\n",
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(mean_std_stat, X, Y),\n",
    "    from_zero=True)  # if your statistic can be negative, remove the from_zero argument..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least for the statistic I picked, this was comfortably inside the rejection region, even though visually the samples are quite different.\n",
    "\n",
    "Of course, it's possible we were just unlucky (as we saw could happen above) \u2013 but actually, the distributions I generated this data from have exactly the same means and standard deviations. Clearly, this test is never going to be able to tell such data apart, so it's time to move onto something better. We could keep adding more and more moments, but those get hard to estimate pretty quickly, so let's remember what we talked about in the lectures instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Mean Discrepancy\n",
    "\n",
    "As we saw in the lectures, the MMD can be thought of as either\n",
    "$$\n",
    "\\MMD(\\PP, \\QQ)\n",
    "= \\sup_{f \\in \\h : \\lVert f \\rVert_\\h \\le 1} \\E_{X \\sim \\PP}[f(X)] - \\E_{Y \\sim \\QQ}[ f(Y) ]\n",
    "$$\n",
    "or, more relevantly right now,\n",
    "$$\n",
    "\\MMD(\\PP, \\QQ)\n",
    "=\n",
    "\\lVert\n",
    "\\E_X[ \\varphi(X) ]\n",
    "- \\E_Y[ \\varphi(Y) ]\n",
    "\\rVert_\\h\n",
    "$$\n",
    "where $\\varphi : \\mathcal X \\to \\h$ is the feature map of the kernel $k(x, y) = \\langle \\varphi(x), \\varphi(y) \\rangle_\\h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take $\\varphi(x) = x$, corresponding to a linear kernel, then our squared difference in means is exactly the squared MMD. But we can do a lot better by picking a different kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** does the difference in means + standard deviations you used above correspond to an MMD with some kernel? If so, what kernel? If not, is there a kernel MMD that can distinguish the same set of distributions as that can?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing kernels\n",
    "\n",
    "To help organize computing kernels, I've put some infrastructure in the `LazyKernel` class (in [`support.kernels`](support/kernels.py), if you want to look at it). It handles a bunch of gruntwork that you won't want to deal with later on. Here's an example of how to use it to implement a kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearKernel(LazyKernel):\n",
    "    def _compute(self, A, B):\n",
    "        return A @ B.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `_compute` method computes the kernel between two matrices of inputs, `A` and `B`, of shape `[n_A, dim]` and `[n_B, dim]`; it returns a kernel matrix of shape `[n_A, n_B]`. (Here, `.t()` is PyTorch syntax for a transpose, and `@` is the nifty Python 3.6+ syntax for matrix multiplication.)\n",
    "\n",
    "The `LazyKernel` base class lets us use this in various ways. First, to find the kernel from one set of points to another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = LinearKernel(X, Y)\n",
    "print(K)\n",
    "K.XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the X-to-X (`XX`) and Y-to-Y (`YY`) kernel matrices from the same object (which are the result of `_compute(X, X)` and `_compute(Y, Y)`). These aren't computed until you need them, but then they're cached after you use them the first time; this is why it's a `LazyKernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want the kernel matrix for a dataset to itself, you can just not pass the second argument. Then `K.XY` won't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearKernel(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass three arguments; then there'll be `XZ`, etc. You can also access them with e.g. `K[0, 2]`.\n",
    "\n",
    "Alternatively, you can pass `None`, which is a special value meaning \"use the first one.\" Then `XY` and so on will exist, but it knows to cache them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = LinearKernel(X, None, Y)\n",
    "print(K)\n",
    "K.YZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a slightly more complex kernel class, with some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialKernel(LazyKernel):\n",
    "    def __init__(self, X, *rest, degree=3, gamma=None, coef0=1):\n",
    "        super().__init__(X, *rest)\n",
    "        self.degree = degree\n",
    "        self.gamma = 1 / X.shape[1] if gamma is None else gamma\n",
    "        self.coef0 = coef0\n",
    "\n",
    "    def _compute(self, A, B):\n",
    "        XY = A @ B.t()\n",
    "        return (self.gamma * XY + self.coef0) ** self.degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also totally *don't need to do this* \u2013\u00a0you definitely won't notice the difference on this scale of data \u2013 but if you want to cache some computation for each dataset, there's a `_precompute` interface. You can return a list of cached information in `_precompute`, which then get passed to `_compute` as `_compute(A, *A_precomputed, B, *B_precomputed)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAndSquareKernel(LazyKernel):\n",
    "    def _precompute(self, A):\n",
    "        return [A * A]\n",
    "    \n",
    "    def _compute(self, A, A_squared, B, B_squared):\n",
    "        return A @ B.t() + A_squared @ B_squared.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're having trouble with the `_compute` matrix interface, you can also use a slower but simpler version where you only `_compute_one` for two entries at a time. This will definitely be noticeably slower; early parts of the notebook will be fine, but it could start being a problem later on.\n",
    "\n",
    "(You don't need to implement `_compute_one` if you implement `_compute`; also, `_compute_one` doesn't support using `_precompute`.)\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAndSquareKernelSlower(LazyKernel):\n",
    "    def _compute_one(self, a, b):\n",
    "        return a @ b + (a * a) @ (b * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LinearAndSquareKernel(X, Y).XY - LinearAndSquareKernelSlower(X, Y).XY).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Remember from lecture that a *characteristic* kernel can distinguish *any* pair of distributions (with enough samples). None of the kernels we've implemented so far are characteristic. Your job is to implement one: the kernel that goes by names including exponentiated quadratic, squared exponential, Gaussian RBF, and probably more:\n",
    "\n",
    "$$k(X_i, X_j) = \\exp\\left( -\\frac{1}{2 \\sigma^2} \\lVert X_i - X_j \\rVert^2 \\right).$$\n",
    "\n",
    "It might be helpful to recall that\n",
    "$$\\lVert X_i - X_j \\rVert^2 = \\lVert X_i \\rVert^2 + \\lVert X_j \\rVert^2 - 2 X_i^T X_j;$$\n",
    "you should probably convert that into a matrix form to implement it in `_compute`, or use `_compute_one` if you're having trouble with that.\n",
    "\n",
    "Alternatively, [`torch.pdist`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pdist) computes these distances, but it only gives you the upper triangle of the matrix so you'll have to turn it into a full symmetric matrix yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpQuadKernel(LazyKernel):\n",
    "    def __init__(self, *parts, sigma=1):\n",
    "        super().__init__(*parts)\n",
    "        self.sigma = sigma\n",
    "        self.const_diagonal = 1  # Says that k(x, x) = 1 for any x.\n",
    "                                 # Just a slight optimization; not really necessary.\n",
    "    \n",
    "    # TODO: implement _compute (maybe with _precompute) or _compute_one\n",
    "    def _precompute(self, A):                                            # SOLUTION\n",
    "        # Squared norms of each data point                               # SOLUTION\n",
    "        return [torch.einsum(\"ij,ij->i\", A, A)]                          # SOLUTION\n",
    "                                                                         # SOLUTION\n",
    "    def _compute(self, A, A_sqnorms, B, B_sqnorms):                      # SOLUTION\n",
    "        D2 = A_sqnorms[:, None] + B_sqnorms[None, :] - 2 * (A @ B.t())   # SOLUTION\n",
    "        return torch.exp(D2 / (-2 * self.sigma ** 2))                    # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your implementation against scikit-learn's implementation (but it doesn't work in PyTorch, so don't just use it directly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.random.lognormal()\n",
    "K = ExpQuadKernel(X, Y, sigma=sigma)\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "gamma = 1 / (2 * sigma**2)  # sklearn uses this parameterization\n",
    "assert np.allclose(K.XX.numpy(), rbf_kernel(X, gamma=gamma))\n",
    "assert np.allclose(K.XY.numpy(), rbf_kernel(X, Y, gamma=gamma))\n",
    "assert np.allclose(K.YY.numpy(), rbf_kernel(Y, gamma=gamma))\n",
    "del rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Matrix` classes\n",
    "\n",
    "Okay, one more handy thing about these `LazyKernel` classes that might be useful here. (You don't have to use them, but you can.)\n",
    "\n",
    "if you do `K.XY_m` (or `K.YY_m` or `K.matrix(0, 1)`, etc), then you get a special `support.kernels.Matrix` subclass. This implements \u2013\u00a0and caches \u2013\u00a0various operations you might need. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = ExpQuadKernel(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XY_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  '.join(m for m in dir(K.XY_m) if not m.startswith('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  '.join(m for m in dir(K.XX_m) if not m.startswith('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX_m.mean(), K.XX_m.offdiag_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the second half of [`support/kernels.py`](support/kernels.py) to see how they're implemented / what options there are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMD estimators\n",
    "\n",
    "Okay, enough admiring my beautiful code. Remember that we have\n",
    "\\begin{align}\n",
    "\\MMD^2(\\PP, \\QQ)\n",
    "  &= \\lVert \\E_X[ \\varphi(X) ] - \\E_Y[ \\varphi(Y) ] \\rVert_\\h^2\n",
    "\\\\&= \\langle\n",
    "        \\E_X[ \\varphi(X) ] - \\E_Y[ \\varphi(Y) ],\n",
    "        \\E_X[ \\varphi(X) ] - \\E_Y[ \\varphi(Y) ]\n",
    "     \\rangle_\\h\n",
    "\\\\&= \\langle \\E_X[ \\varphi(X) ], \\E_X[ \\varphi(X) \\rangle_\\h\n",
    "   + \\langle \\E_Y[ \\varphi(Y) ], \\E_Y[ \\varphi(Y) \\rangle_\\h\n",
    "   - 2 \\langle \\E_X[ \\varphi(X) ], \\E_Y[ \\varphi(Y) ] \\rangle_\\h\n",
    "\\\\&= \\E_{X, X', Y, Y'}\\left[\n",
    "     \\langle \\varphi(X), \\varphi(X') \\rangle_\\h\n",
    "   + \\langle \\varphi(Y), \\varphi(Y') \\rangle_\\h\n",
    "   - 2 \\langle \\varphi(X), \\varphi(Y) \\rangle_\\h\n",
    "   \\right]\n",
    "\\\\&= \\E_{X, X', Y, Y'}\\left[\n",
    "     k(X, X')\n",
    "   + k(Y, Y')\n",
    "   - 2 k(X, Y)\n",
    "   \\right]\n",
    ".\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a pretty natural idea for how to estimate the MMD. Well, three ideas of varying amounts of naturalness:\n",
    "\n",
    "The *biased* estimator (exactly the MMD between the empirical distributions) is, if we have $m$ samples from $X$ and $n$ from $Y$:\n",
    "$$\n",
    "\\MMDhat_b^2(X, Y)\n",
    "= \\frac{1}{m^2} \\sum_{i=1}^m \\sum_{j=1}^m k(X_i, X_j)\n",
    "+ \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n k(Y_i, Y_j)\n",
    "- \\frac{2}{m n} \\sum_{i=1}^m \\sum_{j=1}^n k(X_i, Y_j)\n",
    ".$$\n",
    "But this term has a bias due to the $k(X_i, X_i)$ and $k(Y_i, Y_i)$ terms.\n",
    "\n",
    "(You can tell that it's biased because if $X$ and $Y$ are each samples from the same distribution, the true MMD value is $0$, usually $\\MMDhat^2(X, Y) > 0$, and it's not possible to have $\\MMDhat^2(X, Y) < 0$ \u2013 thus $\\E \\MMDhat^2(X, Y) > 0$.)\n",
    "\n",
    "The *unbiased* estimator gets rid of these terms:\n",
    "$$\n",
    "\\MMDhat_u^2(X, Y)\n",
    "= \\frac{1}{m (m-1)} \\sum_{i \\ne j}^m k(X_i, X_j)\n",
    "+ \\frac{1}{n (n-1)} \\sum_{i \\ne j}^n k(Y_i, Y_j)\n",
    "- \\frac{2}{m n} \\sum_{i=1}^m \\sum_{j=1}^n k(X_i, Y_j)\n",
    ".$$\n",
    "This makes it unbiased; in fact, it's the minimum variance unbiased estimator.\n",
    "\n",
    "The $U$-statistic estimator only works when $n = m$, and also takes out the $k(X_i, Y_i)$ terms, which gives you a slightly worse estimator:\n",
    "$$\n",
    "\\MMDhat_U^2(X, Y)\n",
    "= \\frac{1}{m (m-1)} \\sum_{i \\ne j}^m \\left(\n",
    "    k(X_i, X_j) + k(Y_i, Y_j) - 2 k(X_i, Y_j)\n",
    "  \\right)\n",
    ".$$\n",
    "The advantage is that $U$-statistics have been studied pretty thoroughly by statisticians, so we know things about their variance and asymptotic distributions and whatnot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement at least one of these estimators, as functions of a `LazyKernel(X, Y)`. Using the `Matrix` helpers, each can be literally one line, but you can implement it however you feel like. You don't necessarily have to do all three; do at least one of them, but the others aren't too much extra work on top of the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd2_biased(K):\n",
    "    pass  # TODO: implement, using K.XX_m, K.XY_m, K.YY_m (or K.XX, K.XY, K.YY)\n",
    "    return K.XX_m.mean() + K.YY_m.mean() - 2 * K.XY_m.mean()  # SOLUTION\n",
    "\n",
    "def mmd2_unbiased(K):\n",
    "    pass  # TODO: implement\n",
    "    return K.XX_m.offdiag_mean() + K.YY_m.offdiag_mean() - 2 * K.XY_m.mean()  # SOLUTION\n",
    "\n",
    "def mmd2_u_stat(K):\n",
    "    assert K.ns[0] == K.ns[1]\n",
    "    # TODO: implement\n",
    "    return K.XX_m.offdiag_mean() + K.YY_m.offdiag_mean() - 2 * K.XY_m.offdiag_mean()  # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_biased(ExpQuadKernel(X, Y)), X, Y),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_unbiased(ExpQuadKernel(X, Y)), X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_u_stat(ExpQuadKernel(X, Y)), X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're all pretty much the same; when you're just doing permutation testing, the differences between them aren't super important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Run the test with a `LinearKernel`. Which of our estimators, if any, corresponds to the `mean_difference` statistic from before? (Do you understand why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_u_stat(LinearKernel(X, Y)), X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# Answer: it's mmd2_biased, as you can see e.g. by\n",
    "# expanding k(x, y) = x y in the equation for the sum.\n",
    "# Here's the numerical check:\n",
    "K = LinearKernel(X, Y)\n",
    "torch.stack([\n",
    "    mean_difference(X.squeeze(1), Y.squeeze(1), squared=True),\n",
    "    mmd2_biased(K),\n",
    "    mmd2_unbiased(K),\n",
    "    mmd2_u_stat(K),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** We're working on small datasets here so far, so this implementation of permutations is fine. But notice that we're recomputing the kernel matrix for each permutation, even though the matrices actually have all the same elements (just in jumbled-up order). Implement a faster way that doesn't involve this recomputation.\n",
    "\n",
    "Hint: this is easiest for the biased estimator, which you can write in a way amenable to [this kind of approach](https://pytorch.org/docs/stable/torch.html#torch.einsum). (The `LazyKernel.joint()` method, which concatenates all the kernel matrices together, might be useful.) Doing some algebra to work out a few slightly annoying correction terms, you can also do it similarly for the U-statistic estimator.\n",
    "\n",
    "If you're really, really careful, you can also make the unbiased estimator really fast (exploiting cache locality and things like that). We wrote about this in [this paper](https://arxiv.org/abs/1611.04488) (Section 3), and it's implemented in [Shogun](https://www.shogun-toolbox.org/). (Example usage [here](http://shogun.ml/examples/latest/examples/statistical_testing/quadratic_time_mmd.html) or in in [this notebook](https://nbviewer.jupyter.org/github/shogun-toolbox/shogun/blob/develop/doc/ipython-notebooks/statistical_testing/mmd_two_sample_testing.ipynb).)\n",
    "\n",
    "_Extra credit:_ empirically compare how our silly implementation here, your faster implementation, and the Shogun implementation scale as the dataset size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override this function if you implement a better one yourself,\n",
    "# and the stuff later will run faster....\n",
    "# This just establishes an API for passing in a kernel directly,\n",
    "# which is a little clumsy to implement, but it's not any faster.\n",
    "def mmd2_permutations_slow(K, use_biased=False, num_permutations=1000, progress=True):\n",
    "    # Some fiddling to be able to use the same kernel with \"new\" data.\n",
    "    assert K.n_parts == 2\n",
    "    from copy import copy\n",
    "    K_copy = copy(K)\n",
    "    def mmd2_with_K(X, Y):\n",
    "        K_copy.change_part(0, X)\n",
    "        K_copy.change_part(1, Y)\n",
    "        return (mmd2_biased if use_biased else mmd2_unbiased)(K_copy)\n",
    "    \n",
    "    return two_sample_permutation_test(\n",
    "        mmd2_with_K, K.X, K.Y,\n",
    "        num_permutations=num_permutations, progress=progress)\n",
    "\n",
    "mmd2_permutations = mmd2_permutations_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = ExpQuadKernel(X, Y, sigma=1)\n",
    "plot_null_samples(*mmd2_permutations_slow(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "\n",
    "# For the biased estimator, note that\n",
    "#   MMD_b^2(X, Y) = \\sum_{i,j} w_i Kjoint_{ij} w_j\n",
    "#   with w_i = 1/n_X if i is in X, -1/n_Y if it's in Y.\n",
    "# Then we just do that simultaneously for a bunch of ws.\n",
    "def mmd2_biased_permutations(K, num_permutations=1000, progress=None):\n",
    "    # progress argument is ignored\n",
    "    full_kernel = K.joint()\n",
    "    assert K.n_parts == 2\n",
    "    n_X, n_Y = K.ns\n",
    "    n_tot = n_X + n_Y\n",
    "\n",
    "    # assign permutations; last row is the true split\n",
    "    ws = full_kernel.new_full((num_permutations + 1, n_tot), -1 / n_Y)\n",
    "    ws[-1, :n_X] = 1 / n_X\n",
    "    for i in range(num_permutations):\n",
    "        ws[i, np.random.choice(n_tot, n_X, replace=False)] = 1 / n_X\n",
    "\n",
    "    ests = torch.einsum(\"pi,ij,pj->p\", ws, full_kernel, ws)\n",
    "    return ests[-1], ests[:-1]\n",
    "\n",
    "\n",
    "# The U-statistic case does basically the same thing, but needs\n",
    "# some correction terms:\n",
    "#   MMD_U^2(X, Y) = (\n",
    "#      \\sum_ij K(X_i, X_j) + \\sum_ij K(Y_i, Y_j) - 2 \\sum_ij K(X_i, Y_j)\n",
    "#      - \\sum_i K(X_i, X_i) - \\sum_i K(Y_i, Y_i) + 2 \\sum_i K(X_i, Y_i)\n",
    "#   ) / (n_X * (n_X - 1))\n",
    "def mmd2_u_stat_permutations(K, num_permutations=1000, progress=None):\n",
    "    # progress is ignored\n",
    "    full_kernel = K.joint()\n",
    "    assert K.n_parts == 2\n",
    "    n_X, n_Y = K.ns\n",
    "    assert n_X == n_Y\n",
    "    n_tot = n_X + n_Y\n",
    "\n",
    "    # assign permutations; last row is the true split\n",
    "    ws = full_kernel.new_full((num_permutations + 1, n_tot), -1)\n",
    "    ws[-1, :n_X] = 1\n",
    "    for i in range(num_permutations):\n",
    "        ws[i, np.random.choice(n_tot, n_X, replace=False)] = 1\n",
    "\n",
    "    base_ests = torch.einsum(\"pi,ij,pj->p\", ws, full_kernel, ws)\n",
    "    \n",
    "    # need to find \\sum_i k(X_i, Y_i)\n",
    "    is_X = ws > 0\n",
    "    X_inds = is_X.nonzero()[:, 1].view(num_permutations + 1, n_X)\n",
    "    Y_inds = (~is_X).nonzero()[:, 1].view(num_permutations + 1, n_Y)\n",
    "    cross_terms = full_kernel.take(Y_inds * n_tot + X_inds).sum(1)\n",
    "    \n",
    "    ests = (base_ests - full_kernel.trace() + 2 * cross_terms) / (n_X * (n_X - 1))\n",
    "    return ests[-1], ests[:-1]\n",
    "\n",
    "def mmd2_permutations(K, use_biased=False, num_permutations=1000, progress=None):\n",
    "    if use_biased:\n",
    "        return mmd2_biased_permutations(K, num_permutations=num_permutations, progress=progress)\n",
    "    else:\n",
    "        return mmd2_u_stat_permutations(K, num_permutations=num_permutations, progress=progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "plot_null_samples(*mmd2_permutations(K, use_biased=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative methods to estimate the null distribution\n",
    "\n",
    "There are many other way to get our hands on the distribution of the MMD test statistic under the null hypothesis.\n",
    "\n",
    "We know that asymptotically, the distribution is an infinite sum of Chi-square variables \u2013 but it's an infinite sum, and we don't even know what sum it is, so that doesn't help us too much.\n",
    "Techniques to approximate it include:\n",
    "\n",
    "* moment matching using a Gamma distribution: fast, but doesn't result in a consistent test.\n",
    "* a spectral approximation: using eigenvalues of the kernel matrix, we can estimate that infinite sum. Costly (cubic!) for large sample sets.\n",
    "* wild bootstrap: a technique used for correlated samples that is similar to permuting. (It's useful for e.g. testing an MCMC chain, though.)\n",
    "* linear time statistics: for those, one can often show that the null distribution is Gaussian, with a variance you can estimate in closed form.\n",
    "\n",
    "We won't cover any of these here, since they require some more details.\n",
    "The permutation test, while it can potentially be slow, is easy to understand and works for most applications (if implemented well).\n",
    "\n",
    "[This old Shogun notebook](https://nbviewer.jupyter.org/github/shogun-toolbox/shogun/blob/develop/doc/ipython-notebooks/statistical_testing/mmd_two_sample_testing.ipynb) contains many null approximation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD witness function in the RKHS\n",
    "\n",
    "One nice feature of the MMD is that we can see where the density functions are different by looking at the (empirical) MMD witness function.\n",
    "Remember that\n",
    "\\begin{align}\n",
    "\\MMD(\\PP, \\QQ)\n",
    "  &= \\sup_{f : \\lVert f \\rVert_\\h \\le 1} \\E_{X \\sim \\PP} f(X) - \\E_{Y \\sim \\QQ} f(Y)\n",
    "\\\\&= \\sup_{f : \\lVert f \\rVert_\\h \\le 1} \\langle f, \\E_{X \\sim \\PP}[\\varphi(X)] - \\E_{Y \\sim \\QQ}[\\varphi(Y)] \\rangle_\\h\n",
    "\\end{align}\n",
    "and so\n",
    "$$\n",
    "f^* \\propto \\E_{X \\sim \\PP}[\\varphi(X)] - \\E_{Y \\sim \\QQ}[\\varphi(Y)]\n",
    ",$$\n",
    "which means (using $f^*(t) = \\langle f^*, \\varphi(t) \\rangle_\\h$)\n",
    "$$\n",
    "f^*(t) \\propto \\E_{X \\sim \\PP} k(X, t) - \\E_{Y \\sim \\QQ} k(Y, t)\n",
    ".$$\n",
    "\n",
    "We can estimate $f^*$ with empirical averages.\n",
    "\n",
    "**Thought exercises:**\n",
    "- What's the proportionality constant hidden by $\\propto$?\n",
    "- Does the constant matter?\n",
    "- Which MMD estimator, if any, does directly estimating $f^*$ correspond to?\n",
    "\n",
    "The points where $\\lvert f^*(t) \\rvert$ are large are where the MMD test considers $\\PP$ and $\\QQ$ to be the \"most different.\"\n",
    "Let's define a grid and evaluate the witness function on it to see where that is.\n",
    "\n",
    "**Exercise:** Finish the `mmd_witness` function to evaluate the witness function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd_witness(K, eval_pts):\n",
    "    assert K.n_parts == 2\n",
    "    K.append_part(eval_pts[:, None])  # now K.XZ compares X to eval_pts\n",
    "    \n",
    "    witness = 0\n",
    "    # TODO: estimate MMD witness function on grid_pts\n",
    "    witness = K.XZ.mean(0) - K.YZ.mean(0)                                    # SOLUTION\n",
    "    witness = witness / mmd2_biased(K)  # to normalize; maybe not necessary  # SOLUTION\n",
    "    \n",
    "    K.drop_last_part()\n",
    "    return witness\n",
    "\n",
    "\n",
    "def plot_mmd_witness_1d(K, ax=None, grid_num=1000):\n",
    "    X, Y = [t.squeeze(1) for t in K.parts]\n",
    "    assert len(X.shape) == 1\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.hist(X.numpy(), alpha=.5, density=True, histtype='stepfilled', bins='auto')\n",
    "    ax.hist(Y.numpy(), alpha=.5, density=True, histtype='stepfilled', bins='auto')\n",
    "    ax.grid(False)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    lo, hi = ax2.get_xlim()\n",
    "    grid_pts = torch.linspace(lo, hi, grid_num)\n",
    "    ax2.plot(grid_pts.numpy(), mmd_witness(K, grid_pts).numpy(), color='k', lw=2)\n",
    "    ax2.set_xlim(lo, hi)\n",
    "    ax2.grid(False)\n",
    "    ax2.axhline(0, color='k', lw=0.5)\n",
    "    ax2.set_ylabel(\"Witness value\")\n",
    "    \n",
    "plot_mmd_witness_1d(ExpQuadKernel(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the witness function is positive where X as a higher density than Y, and negative vice versa.\n",
    "It is zero where both densities match.\n",
    "Intuitively, the RKHS norm of this function can only be zero if the densities match everywhere, and it grows as the densities differ on more and more points in their support.\n",
    "Of course, this kind of visualization only works in low dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a kernel\n",
    "\n",
    "So far, we've been using `ExpQuadKernel` with its default `sigma=1`. That's a pretty arbitrary choice; what if we tried some different values?\n",
    "\n",
    "**Thought exercise:** What do you expect the test results and witness functions to look like if we try larger or smaller sigmas (as the next block does)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sig in [.1, .001, 10]:\n",
    "    K = ExpQuadKernel(X, Y, sigma=sig)\n",
    "    fig, (a1, a2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "    plot_null_samples(*mmd2_permutations(K, use_biased=False), ax=a1)\n",
    "    plot_mmd_witness_1d(K, ax=a2)\n",
    "    fig.suptitle(f\"$\\sigma$ = {sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you intuitively understand what happened here (especially the witness plots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, bandwidth 1 is a totally arbitrary choice that depends on the scaling of the data. We're going to talk about how to make actual, data-informed choices in a minute. But first, it's a good idea to start with a heuristic that usually works:\n",
    "\n",
    "[![](figs/dril-heuristic.png)](https://twitter.com/dril/status/484722159462260736)\n",
    "\n",
    "In our case, the choice we can try evbery time is the \"median heuristic\": set $\\sigma$ to the median of the distance between data points.\n",
    "\n",
    "**Exercise:** compute the median distance between training data points. You can either compute this yourself, or use predefined functions, whichever you'd prefer. If it's too slow to compute, you can subsample the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_distance(Z):\n",
    "    # TODO: return the median distance among the stacked samples Z\n",
    "    # If you want to be fancy, add options to optionally subset if Z is big.\n",
    "    return torch.median(torch.pdist(Z))  # SOLUTION\n",
    "\n",
    "med = median_distance(torch.cat([X, Y], 0))\n",
    "K = ExpQuadKernel(X, Y, sigma=med)\n",
    "\n",
    "fig, (a1, a2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "plot_null_samples(*mmd2_permutations(K), ax=a1, one_sided=True)\n",
    "plot_mmd_witness_1d(K, ax=a2)\n",
    "\n",
    "med.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the default 1 wasn't too far off, actually. The median heuristic worked great!\n",
    "\n",
    "But it doesn't always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dataset where the median heuristic doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def sample_blobs(n=500, ratio=0.01, rows=10, cols=10, sep=10, rs=None):\n",
    "    rs = check_random_state(rs)\n",
    "    # ratio is eigenvalue ratio\n",
    "    correlation = (ratio - 1) / (ratio + 1)\n",
    "\n",
    "    # generate within-blob variation\n",
    "    mu = np.zeros(2)\n",
    "    sigma = np.eye(2)\n",
    "    X = rs.multivariate_normal(mu, sigma, size=n)\n",
    "\n",
    "    corr_sigma = np.array([[1, correlation], [correlation, 1]])\n",
    "    Y = rs.multivariate_normal(mu, corr_sigma, size=n)\n",
    "\n",
    "    # assign to blobs\n",
    "    X[:, 0] += rs.randint(rows, size=n) * sep\n",
    "    X[:, 1] += rs.randint(cols, size=n) * sep\n",
    "    Y[:, 0] += rs.randint(rows, size=n) * sep\n",
    "    Y[:, 1] += rs.randint(cols, size=n) * sep\n",
    "\n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "X, Y = sample_blobs(rs=0)\n",
    "np.savez('data/blobs.npz', X=X, Y=Y)\n",
    "\n",
    "X, Y = sample_blobs(rs=1)\n",
    "np.savez('data/blobs2.npz', X=X, Y=Y)\n",
    "\n",
    "X, Y = sample_blobs(rs=2, rows=1, cols=1)\n",
    "np.savez('data/blobs_single.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "\n",
    "with np.load('data/blobs.npz') as d:\n",
    "    X, Y = as_tensors(d['X'], d['Y'])\n",
    "ax1.scatter(*X.t(), alpha=.5, label='X')\n",
    "ax1.scatter(*Y.t(), alpha=.5, label='Y')\n",
    "# ax1.legend()\n",
    "ax1.set_title(\"Blobs\")\n",
    "\n",
    "with np.load('data/blobs_single.npz') as d:\n",
    "    ax2.scatter(*d['X'].T, alpha=0.5, label='X')\n",
    "    ax2.scatter(*d['Y'].T, alpha=0.5, label='Y')\n",
    "ax2.legend()\n",
    "ax2.set_title(\"One blob (zoomed in)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two distributions are clearly very different. But the scale at which they're different is much smaller than the median distance, which looks mostly at global structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = median_distance(torch.cat([X, Y], 0))\n",
    "K = ExpQuadKernel(X, Y, sigma=med)\n",
    "\n",
    "plot_null_samples(*mmd2_permutations(K))\n",
    "med.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning kernels by maximizing (a proxy for) test power\n",
    "\n",
    "So, we want some way to find the best test, where \"best\" means \"most powerful.\" That is, we want some criterion where we can get a sense of how powerful a test is \u2013 preferably without having to run the test lots of times, since that'll take lots of data and computation.\n",
    "\n",
    "There are a few possible schemes to do this. Here we're going to consider one that depends on a little math.\n",
    "\n",
    "It turns out that under the alternative $\\PP \\ne \\QQ$,\n",
    "$\\MMDhat_U^2$ is asymptotically normal,\n",
    "$$\n",
    "\\frac{\\MMDhat_U^2(X, Y) - \\MMD^2(\\PP, \\QQ)}{\\sqrt{\\Var_{X' \\sim \\PP^m, Y' \\sim \\QQ^m}\\left[ \\MMDhat_U^2(X', Y') \\right]}}\n",
    "\\stackrel{D}{\\to}\n",
    "\\mathcal N(0, 1)\n",
    ".$$\n",
    "Call that thing in the denominator,\n",
    "which is the variance of $\\MMDhat_U^2$ given samples of size $m$ from $\\PP$ and $\\QQ$,\n",
    "$V_m(\\PP, \\QQ)$.\n",
    "\n",
    "Under the null hypothesis $\\PP = \\QQ$,\n",
    "we have that $m \\MMDhat_U^2(X, Y)$ converges to some gross distribution (an infinite mixture of chi-squares).\n",
    "Let $c_\\alpha$ be the $(1-\\alpha)$th quantile of that distribution,\n",
    "the rejection threshold;\n",
    "we don't know that exactly,\n",
    "but we can estimate it with permutation testing as $\\hat c_\\alpha$.\n",
    "In this framework, note that $c_\\alpha$ depends on $\\PP$ and $\\QQ$ (and the kernel in the MMD)\n",
    "but it *doesn't* depend on $m$,\n",
    "and in the usual setting we have\n",
    "$\\hat c_\\alpha \\to c_\\alpha$.\n",
    "\n",
    "So, the power of our test \u2013\u00a0the probability of rejecting when $\\PP \\ne \\QQ$ \u2013 is just\n",
    "\n",
    "\\begin{align}\n",
    "     \\Pr\\left( m \\MMDhat_U^2(X, Y) > \\hat c_\\alpha \\right)\n",
    "  &= \\Pr\\left(\n",
    "       \\frac{\\MMDhat_U^2(X, Y) - \\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "     > \\frac{\\hat c_\\alpha / m - \\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "   \\right)\n",
    "\\\\&\\to 1 - \\Phi\\left(\n",
    "        \\frac{c_\\alpha / m - \\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "    \\right)\n",
    "\\\\&= \\Phi\\left(\n",
    "      \\frac{\\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "    - \\frac{c_\\alpha}{m \\sqrt{V_m(\\PP, \\QQ)}}\n",
    "    \\right)\n",
    "\\end{align}\n",
    "\n",
    "We can then maximize the power of the test by maximizing the thing inside the parentheses.\n",
    "\n",
    "Now, it turns out that for a fixed kernel, $V_m$ is $\\mathcal{O}(1 / m)$, and $\\MMD^2$ and $c_\\alpha$ are constants. So the first term is $\\mathcal{O}\\left(\\sqrt{m}\\right)$ and the second is $\\mathcal{O}\\left(1 / \\sqrt{m}\\right)$ \u2013\u00a0which means we should be able to just ignore the second term when $m$ is reasonably large.\n",
    "\n",
    "That means we need to choose $k$ to maximize\n",
    "$$t := \\frac{\\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}.$$\n",
    "This is a function of the kernel and $\\PP$ and $\\QQ$, *not* our particular sample from it, but we can estimate $t$ based on samples. We know how to estimate $\\MMD^2$, and it turns out you can also estimate $V_m$ in the same quadratic time: see [this note](https://arxiv.org/abs/1906.02104). Then you can just divide the estimators.\n",
    "That note is 12 pages of dense equations, so...I'll be here when you're done.\n",
    "\n",
    "(Don't actually read it.)\n",
    "\n",
    "**Bonus points:** Find a mistake in the note. I checked it three times, but I wouldn't be at all surprised if there are still mistakes!\n",
    "\n",
    "**Big bonus points:** Figure out a way to simplify the derivation. Seriously, huge bonus points.\n",
    "\n",
    "Anyway, even the _expression_ for the estimator is too disgusting to write here. But I implemented it for you, in [`support.mmd2_u_stat_variance`](support/mmd.py). (*That's* what the `Matrix` subclasses are really for.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from support import mmd2_u_stat_variance\n",
    "# mmd2_u_stat_variance(K) estimates V_m based on K.XX, K.XY, K.YY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do permutation testing with kernel selection, we have two choices:\n",
    "\n",
    "- Do permutations of the *whole* test procedure, including picking a kernel. This is valid, but usually super computationally expensive.\n",
    "\n",
    "- Fit the kernel on a training set, and then run the final test on a held-out test set. (If we pick the kernel on the same set we test on, then we're likely to overfit: to pick a kernel that happens to work well on this particular dataset, but not the whole distribution.)\n",
    "\n",
    "In real life, if `X` and `Y` were all we had, we'd have to split them up (and guess at how much to split them by...) into a \"training\" and a \"testing\" set.\n",
    "\n",
    "**Quick thought exercise:** What are the tradeoffs when picking a bigger training set versus a bigger testing set? When would one be a better choice? Can you try multiple values and see?\n",
    "\n",
    "<p style=\"color: blue\"><b>Answer:</b> If the kernel parameter is relatively unimportant, or easy to pick, then a bigger testing set will give you more of a chance to identify the difference in the data. But a bigger training set will give you more ability to pick the best kernel. Unfortunately, you can only try one split (unless you correct for multiple testing).</p> <!-- SOLUTION -->\n",
    "\n",
    "Because I'm generous, I'm going to provide you with another set of samples to learn a kernel on. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/blobs2.npz') as d:\n",
    "    X_train, Y_train = as_tensors(d['X'], d['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compare a fixed set of kernels, the different bandwidths on the blobs data, using this criterion.\n",
    "\n",
    "This is an unbiased estimator, so it might be negative, or zero! Make sure you don't divide by zero, or take the square root of a negative number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_est(K, eps=1e-6):\n",
    "    # TODO: return an estimate of the power criterion above\n",
    "    # You can use eps to avoid dividing by zero, as you please.\n",
    "    eps, = K.as_tensors(eps)\n",
    "    return mmd2_u_stat(K) / torch.max(eps, mmd2_u_stat_variance(K)).sqrt()  # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the function runs\n",
    "criterion_est(ExpQuadKernel(X_train, Y_train, sigma=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate criterion for a bunch of sigmas\n",
    "sigmas = np.logspace(-1.5, 2.5, num=50)\n",
    "t_ests = [criterion_est(ExpQuadKernel(X_train, Y_train, sigma=s)) for s in tqdm(sigmas)]\n",
    "\n",
    "plt.plot(sigmas, t_ests, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.axvline(med, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** We know how to estimate $c_\\alpha$ \u2013\u00a0with permutation testing \u2013 so we could optimize the more-complete criterion, too. Does that give you different answers?\n",
    "\n",
    "([`torch.kthvalue`](https://pytorch.org/docs/stable/torch.html#torch.kthvalue) might be useful. Also, remember that $c_\\alpha$ is the permutation threshold for the distribution of $m \\, \\MMDhat^2$, not just plain $\\MMDhat^2$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "\n",
    "def full_criterion_est(K, level=0.05, eps=1e-6, num_permutations=500, **kwargs):\n",
    "    eps, = K.as_tensors(eps)\n",
    "    mmd2, nulls = mmd2_permutations(K, use_biased=True, num_permutations=num_permutations, **kwargs)\n",
    "    thresh_over_m, _ = nulls.kthvalue(int(num_permutations * (1 - level)))\n",
    "    num = mmd2 - thresh_over_m\n",
    "    den = torch.max(eps, mmd2_u_stat_variance(K)).sqrt()\n",
    "    return num / den\n",
    "\n",
    "sub_sigmas = sigmas[::2]  # be a little faster\n",
    "full_ests = [full_criterion_est(ExpQuadKernel(X_train, Y_train, sigma=s)) for s in tqdm(sub_sigmas)]\n",
    "\n",
    "plt.plot(sigmas, t_ests, marker='o', label='t')\n",
    "plt.plot(sub_sigmas, full_ests, marker='x', label='full')\n",
    "plt.axvline(med, color='r')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent to optimize the power criterion\n",
    "\n",
    "What if we have more than one or two kernel parameters? What if we have dozens, or even (if we have really big datasets) thousands, or millions? Then trying out a grid of different combinations isn't going to work well. Instead, we can be good deep learners and do gradient descent.\n",
    "\n",
    "(This is why we did everything in PyTorch!)\n",
    "\n",
    "**Exercise:** Instead of doing a grid search, optimize $\\sigma$ with gradient descent.\n",
    "\n",
    "It's probably a better idea to parameterize it with $\\log \\sigma$ instead of $\\sigma$ directly.\n",
    "\n",
    "Does it always go to the same place, or can you get stuck in local minima?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sigma = torch.tensor(np.log(1), requires_grad=True)\n",
    "opt = torch.optim.SGD([log_sigma], lr=.1)  # could fiddle with optimizer if you want\n",
    "\n",
    "trace = []\n",
    "with tqdm(range(500)) as bar:  # <- feel free to increase the number of iterations\n",
    "    for i in bar:\n",
    "        opt.zero_grad()  # reset gradient state before computing things\n",
    "        \n",
    "        sigma = torch.exp(log_sigma)\n",
    "        \n",
    "        # TODO: compute into `loss` here, as the thing you want to *minimize*\n",
    "        \n",
    "        # Ideally, our kernel class would be able to cache                       # SOLUTION\n",
    "        # distances between iterations here since we're only changing sigma...   # SOLUTION\n",
    "        # but it can't.                                                          # SOLUTION\n",
    "        K = ExpQuadKernel(X_train, Y_train, sigma=sigma)                         # SOLUTION\n",
    "        t_est = criterion_est(K)                                                 # SOLUTION\n",
    "        loss = -t_est  # could add regularization here, etc                      # SOLUTION\n",
    "        \n",
    "        loss.backward()  # compute the gradients\n",
    "        opt.step()       # update log_sigma along its gradient\n",
    "        \n",
    "        sig_val = sigma.cpu().item()\n",
    "        loss_val = loss.cpu().item()\n",
    "        \n",
    "        trace.append((i, sig_val, loss_val))\n",
    "        bar.set_postfix(sigma=f\"{sig_val:.4}\", loss=f\"{loss_val:.4}\")\n",
    "\n",
    "fig, (a1, a2) = plt.subplots(ncols=2, figsize=(12, 4), constrained_layout=True)\n",
    "inds, sigs, losses = np.asarray(trace).T\n",
    "\n",
    "a1.plot(inds, losses)\n",
    "a1.set_title(\"loss estimate\")\n",
    "a1.set_xlabel(\"iteration\")\n",
    "\n",
    "a2.plot(inds, sigs)\n",
    "a2.set_title(r\"$\\sigma$\")\n",
    "a2.set_xlabel(\"iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN criticism with an ARD kernel\n",
    "\n",
    "You may have heard of Generative Adversarial Networks, GANs.\n",
    "Assume someone came to you with a new GAN, which generates some realistic looking images, which they say are indistinguishable from the images used to train the GAN.\n",
    "\n",
    "Actually, let's assume that person [said](https://arxiv.org/abs/1606.03498):\n",
    "> On MTurk, annotators were able to distinguish\n",
    "samples in 52.4% of cases (2000 votes total), where 50% would be obtained by random\n",
    "guessing. Similarly, researchers in our institution were not able to find any artifacts that would allow\n",
    "them to distinguish samples.\n",
    "\n",
    "Let's see whether we can do better than the MTurk annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/gan-samples.npz') as d:\n",
    "    X, Y = [t.float() for t in as_tensors(d['X'], d['Y'])]\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These samples are 0-1 valued, because otherwise it's too easy to trivially\n",
    "# distinguish them.\n",
    "X.unique(), Y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"X (true MNIST) samples:\"))\n",
    "display(pil_grid(X[:40].reshape(-1, 1, 28, 28), nrow=20))\n",
    "display(Markdown(\"Y (GAN) samples:\"))\n",
    "display(pil_grid(Y[:40].reshape(-1, 1, 28, 28), nrow=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` and `Y` each contain 10,000 points. It turns out to be trivial to distinguish the two distributions if you allow yourself to look at all 10,000 at once. Our MMD tests would construct a few 10,000 $\\times$ 10,000 kernel matrices, which would be slow, but we can use a simple $\\lVert \\hat\\mu_X - \\hat\\mu_Y \\rVert^2$ statistic \u2013 equivalent to `mmd2_biased` with `LinearKernel` \u2013 to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_diff_sqnorm(X, Y):\n",
    "    X, Y = as_tensors(X, Y)\n",
    "    return (X.mean(0) - Y.mean(0)).pow(2).sum()\n",
    "\n",
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(mean_diff_sqnorm, X, Y, num_permutations=100),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is enough to very confidently say that $\\PP$ and $\\QQ$ have different means. Looking at 10,000 samples at a time, though, is a lot more than a human can realistically do.\n",
    "\n",
    "**Exercise:** Can this linear-kernel MMD reliably distinguish $\\PP$ and $\\QQ$ based on only 10 samples at a time?\n",
    "\n",
    "**Optional:** Estimate the power of the test at a level of $\\alpha = 0.1$ with 10 or 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# First, to just try it once:\n",
    "\n",
    "Xsub = X[np.random.choice(X.shape[0], 10, replace=False)]\n",
    "Ysub = Y[np.random.choice(Y.shape[0], 10, replace=False)]\n",
    "\n",
    "plot_null_samples(\n",
    "    *two_sample_permutation_test(mean_diff_sqnorm, Xsub, Ysub),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# To estimate the power, do it a bunch of times.\n",
    "# Because we'll only do it with small subset_size,\n",
    "# only bother implementing for mmd2_permutations.\n",
    "\n",
    "def p_value(statistic, null_samples):\n",
    "    statistic, null_samples = as_tensors(statistic, null_samples)\n",
    "    return (statistic < null_samples).float().mean()\n",
    "\n",
    "def subset_mmd_p_values(K_cls, X, Y, subset_size, n_trials=100, num_permutations=500):\n",
    "    return torch.stack([\n",
    "        p_value(*mmd2_permutations(\n",
    "            K_cls(\n",
    "                X[np.random.choice(X.shape[0], subset_size, replace=False)],\n",
    "                Y[np.random.choice(Y.shape[0], subset_size, replace=False)],\n",
    "            ),\n",
    "            num_permutations=num_permutations,\n",
    "            progress=False,\n",
    "        ))\n",
    "        for _ in tqdm(range(n_trials))\n",
    "    ])\n",
    "\n",
    "def plot_power_estimate(ps, levels=[0.1], ax=None, extra=\"\", **hist_kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ps = torch.as_tensor(ps)\n",
    "    \n",
    "    hist_kwargs.setdefault('histtype', 'stepfilled')\n",
    "    hist_kwargs.setdefault('bins', np.linspace(0, 1, num=21))\n",
    "    ax.hist(ps.numpy(), **hist_kwargs)\n",
    "    \n",
    "    powers = []\n",
    "    for level in levels:\n",
    "        ax.axvspan(0, level, color='k', alpha=.1)\n",
    "        ax.axvline(level, color='k', ls='--')\n",
    "        powers.append((ps < level).float().mean())\n",
    "    \n",
    "    ax.set_title(\n",
    "        f\"{extra}{': ' if extra else ''}Power{'s' if len(levels) > 1 else ''} \"\n",
    "        + \", \".join(\n",
    "            f\"{power:.0%} at {level:.0%}\" for level, power in zip(levels, powers)\n",
    "        )\n",
    "    )\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "def compare_powers(K_cls, sizes=[10, 100], hist_kwargs={}, levels=[.01, .1], title=None, **pval_kwargs):\n",
    "    fig, (axes,) = plt.subplots(ncols=len(sizes), figsize=(8 * len(sizes), 4), squeeze=False)\n",
    "    \n",
    "    for sz, ax in zip(sizes, axes):\n",
    "        plot_power_estimate(\n",
    "            subset_mmd_p_values(K_cls, X, Y, subset_size=sz, **pval_kwargs),\n",
    "            ax=ax, extra=f\"Subsets size {sz}\", levels=levels, **hist_kwargs)\n",
    "        \n",
    "    fig.suptitle(getattr(K_cls, '__name__', str(K_cls)) if title is None else title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_powers(LinearKernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How does a median-heuristic exponentiated quadratic kernel perform when given 10 samples at a time? 100?\n",
    "\n",
    "(Technically, the median heuristic should either be inside the permutation process, or else done on a separate training set. Feel free to do the median just once on a decent-sized subset of `X` and `Y`, and don't both enforcing that you don't use that data again \u2013 it's not going to be able to overfit from just that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "\n",
    "med = median_distance(torch.cat([\n",
    "    X[np.random.choice(X.shape[0], 1000, replace=False)],\n",
    "    Y[np.random.choice(Y.shape[0], 1000, replace=False)],\n",
    "], 0)).item()\n",
    "\n",
    "compare_powers(functools.partial(ExpQuadKernel, sigma=med), title=f\"median bandwidth - {med:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now the fun part: let's get a stronger class of kernel, to crank up that power on very small subsets.\n",
    "\n",
    "We can do better with an ARD (\"automatic relevance determination\") kernel.\n",
    "You can think of this kernel as scaling each of the $D$ input dimensions by a different parameter, and then uses a standard Gaussian kernel (unit bandwidth) on top of that.\n",
    "$$ k(x,y) = \\exp\\left(-\\frac12 \\sum_{d=1}^D \\left( s_d x_d - s_d y_d \\right)^2\\right).$$\n",
    "\n",
    "**Exercise:** Implement `_compute` for the `ARDKernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARDKernel(LazyKernel):\n",
    "    def __init__(self, *parts, scales=1):\n",
    "        super().__init__(*parts)\n",
    "        assert len(self.X.shape) == 2\n",
    "        d = self.X.shape[1]\n",
    "        \n",
    "        scales, = self.as_tensors(scales)\n",
    "        if scales.shape == (): # scalar, like the default value 1\n",
    "            scales = scales.repeat(d)\n",
    "        else:\n",
    "            assert scales.shape == (d,)\n",
    "\n",
    "        self.scales = scales\n",
    "        self.const_diagonal = 1  # Says that k(x, x) = 1 for any x\n",
    "    \n",
    "    # TODO: implement _compute (maybe with _precompute)\n",
    "    def _precompute(self, A):                                          # SOLUTION\n",
    "        s = A * self.scales[None, :]                                   # SOLUTION\n",
    "        return [s, torch.einsum(\"id,id->i\", s, s)]                     # SOLUTION\n",
    "                                                                       # SOLUTION\n",
    "    def _compute(self, A, sc_A, sc_A_sqnorms, B, sc_B, sc_B_sqnorms):  # SOLUTION\n",
    "        D2 = (sc_A_sqnorms[:, None] + sc_B_sqnorms[None, :]            # SOLUTION\n",
    "              - 2 * (sc_A @ sc_B.t()))                                 # SOLUTION\n",
    "        return torch.exp(-0.5 * D2)                                    # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that ARDKernel at least agrees with ExpQuadKernel in the special case:\n",
    "Xtmp = torch.randn(3, 4)\n",
    "sigma = torch.exp(torch.randn(()))\n",
    "(ARDKernel(Xtmp, scales=1/sigma).XX\n",
    " - ExpQuadKernel(Xtmp, sigma=sigma).XX).abs().max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Split the data into train and test halves. Optimize the weights of an ARD kernel with gradient descent on the training data, to get the most powerful kernel; then estimate the power with those same small subsets on the test half of the data.\n",
    "\n",
    "By the way, ideally we'd want a kernel that works well for small sample sizes; it's feasible that a kernel that works well for small samples wouldn't work well for large ones. But the criterion\n",
    "$$\n",
    "t = \\frac{\\MMD^2(\\PP, \\QQ)}{\\Var_{X \\sim \\PP^m, Y \\sim \\QQ^m}\\left[\\MMDhat^2(X, Y)\\right]}\n",
    "$$\n",
    "depends on $m$ only in the denominator,\n",
    "and that dependence is *very* close to being simply scaling a constant by $1/m$.\n",
    "Thus the criterion doesn't really depend on $m$ in a meaningful way.\n",
    "So, you should just use a pretty big sample size when choosing a kernel on the training set,\n",
    "to estimate $t$ better,\n",
    "then evaluate the power with small $m$.\n",
    "\n",
    "<small>(The dependence for small $m$ shows up in two places. One is in the $c_\\alpha / m$ term we dropped, which we could potentially include in the optimization, though it means we need to permute at every step. But the main one is just that the null distribution of $m \\, \\MMDhat^2$ is non-normal for small $m$, so the criterion itself is flawed in that case.)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "\n",
    "# X and Y are already in random order.\n",
    "# could shuffle them first if they weren't...\n",
    "X_train, X_test = X.split([X.shape[0] // 2, X.shape[0] - X.shape[0] // 2])\n",
    "Y_train, Y_test = Y.split([Y.shape[0] // 2, Y.shape[0] - Y.shape[0] // 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledLinearKernel(LazyKernel):\n",
    "    def __init__(self, *parts, scales=1):\n",
    "        super().__init__(*parts)\n",
    "        assert len(self.X.shape) == 2\n",
    "        d = self.X.shape[1]\n",
    "        \n",
    "        scales, = self.as_tensors(scales)\n",
    "        if scales.shape == (): # scalar, like the default value 1\n",
    "            scales = scales.repeat(d)\n",
    "        else:\n",
    "            assert scales.shape == (d,)\n",
    "\n",
    "        self.scales = scales\n",
    "    \n",
    "    # TODO: implement _compute (maybe with _precompute)\n",
    "    def _precompute(self, A):              # SOLUTION\n",
    "        return [A * self.scales[None, :]]  # SOLUTION\n",
    "                                           # SOLUTION\n",
    "    def _compute(self, A, sc_A, B, sc_B):  # SOLUTION\n",
    "        return sc_A @ sc_B.t()             # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get errors about torch.cuda.FloatTensor vs torch.FloatTensor, then:\n",
    "#    (a) Make sure you're using .to(device) on the subsetted data you load\n",
    "#        in the training loop.\n",
    "#    (b) Make sure that if you make any temporary variables in the functions\n",
    "#        you call inside the training loop, they inherit their device from\n",
    "#        other variables. You can do this either by explicitly setting\n",
    "#        device=X.device, or use X.new_full() / etc, or use .to(X.device).\n",
    "#    (c) The CPU won't be that much slower than the GPU, probably; you can\n",
    "#        just force device='cpu'.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "scales = torch.full([X.shape[1]], 1, requires_grad=True, device=device)\n",
    "opt = torch.optim.Adam([scales], lr=1e-2)  # feel free to fiddle with this\n",
    "\n",
    "def sub(X, batch_size=100):                                                            # SOLUTION\n",
    "    return X[np.random.choice(X.shape[0], batch_size, replace=False)].to(device)        # SOLUTION\n",
    "\n",
    "trace = []\n",
    "with tqdm(range(1_000)) as bar:  # feel free to change number of iterations...\n",
    "    for i in bar:\n",
    "        opt.zero_grad()  # reset gradient state before computing things\n",
    "        \n",
    "        # TODO: compute into `loss` here, as the thing you want to *minimize*\n",
    "        K = ScaledLinearKernel(sub(X_train), sub(Y_train), scales=scales)      # SOLUTION\n",
    "        t_est = full_criterion_est(K, num_permutations=100)                    # SOLUTION\n",
    "        loss = -t_est\n",
    "#         loss = -torch.log(t_est)                                               # SOLUTION\n",
    "        \n",
    "        loss.backward()  # compute the gradients\n",
    "        opt.step()       # update log_scales along its gradient\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            scales_val = scales.cpu().detach().numpy()\n",
    "            loss_val = loss.cpu().item()\n",
    "        \n",
    "            trace.append((i, scales_val, loss_val))\n",
    "            bar.set_postfix(loss=f\"{loss_val:.4}\")\n",
    "        \n",
    "        if i % 250 == 0:\n",
    "            bar.write(f\"{i:>8,}: {loss_val: .4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, a1 = plt.subplots(ncols=1, figsize=(12, 4), constrained_layout=True)\n",
    "inds, scale_vals, loss_vals = [np.asarray(x) for x in zip(*trace)]\n",
    "\n",
    "a1.plot(inds, np.exp(-loss_vals))\n",
    "a1.set_title(\"t criterion estimate\")\n",
    "a1.set_xlabel(\"iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How powerful is the kernel you learned at 10 or 100 samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_powers(LinearKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_powers(functools.partial(ScaledLinearKernel, scales=scale_vals[-1]), title=\"Scaled linear kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scs = np.abs(scale_vals[-1])\n",
    "plt.imshow(scs.reshape(28, 28)  )#, norm=mpl.colors.LogNorm(vmin=max(1e-4, scs.min()), vmax=scs.max()))\n",
    "plt.grid(False)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('/nfs/nhome/live/dougals/mmdnet/src_alt/results/gan/mnist/ard_maxratio_gray_trim_clip_disc2.npz',\n",
    "             encoding='bytes', allow_pickle=True) as d:\n",
    "    pre_scales = d['params'][0] / np.exp(d['params'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_powers(LinearKernel, sizes=[2000], levels=[.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_powers(functools.partial(ExpQuadKernel, sigma=med), sizes=[100], levels=[.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_powers(functools.partial(ARDKernel, scales=pre_scales), sizes=[100], levels=[.01], title='from paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(pre_scales.reshape(28, 28))\n",
    "plt.grid(False)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** Can you visualize and interpret the weights `scales`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** We can't do the 1d witness function plot, but can you think of a different way to figure out \"where\" the distributions are different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence testing\n",
    "\n",
    "So far, we've been asking whether $\\PP = \\QQ$ for two different distributions. Sometimes we'd rather do a related problem, *independence testing*. That is, we have paired data $\\{ (X_i, Y_i) \\}_{i=1}^n \\sim \\PP_{XY}^n$, and we want to know whether $X$ and $Y$ are independent or not.\n",
    "\n",
    "By definition, this is the same thing as asking whether $\\PP_{XY} = \\PP_X \\times \\PP_Y$, the product of the marginal distributions. But we know how to do that: if we use a characteristic kernel on the joint space, they're independent if and only if $\\MMD(\\PP_{XY}, \\PP_{X} \\times \\PP_Y) = 0$.\n",
    "The quantity $\\MMD(\\PP_{XY}, \\PP_{X} \\times \\PP_Y)^2$  is also called the HSIC (\"Hilbert-Schmidt Independence Criterion\"). \n",
    "\n",
    "Usually we choose a kernel $k$ for $X$ and a kernel $\\ell$ for $Y$, and use the _product kernel_ on the joint space:\n",
    "$$\n",
    "  (k \\times \\ell)( (x, y), (x', y') )\n",
    "  = k(x, x') \\ell(y, y')\n",
    ".$$\n",
    "It turns out you can estimate it like this:\n",
    "$$\n",
    "\\frac{1}{n^2} \\Tr\\left( H K H L \\right)\n",
    ",$$\n",
    "where $K$ is the kernel matrix on the $X$ samples,\n",
    "$L$ is the kernel matrix on the $L$ samples,\n",
    "and $H = I - \\frac1n \\mathbf{1} \\mathbf{1}^T$ is the _centering matrix_.\n",
    "(Here $I$ is the identity matrix, and $\\mathbf 1$ is a vector of all $1$s.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the HSIC estimator. You can do it literally as written here, with an H matrix and taking matrix products and so on. Or, if you want a more efficient implementation \u2013\u00a0$\\mathcal{O}(n^2)$ instead of $\\mathcal{O}(n^3)$ \u2013 you can notice a few things:\n",
    "\n",
    "- If you expand out what $H K H$ does, it can be done in quadratic time without actually constructing $H$.\n",
    "- $\\Tr(A B) = \\sum_i ( A B )_{ii} = \\sum_i \\sum_j A_{ij} B_{ji}$, which you can easily implement in quadratic time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_kernel_matrices(K, L):\n",
    "    if isinstance(K, LazyKernel):\n",
    "        K = K.XX\n",
    "    if isinstance(L, LazyKernel):\n",
    "        L = L.XX\n",
    "    K, L = as_tensors(K, L)\n",
    "    assert len(K.shape) == len(L.shape) == 2\n",
    "    assert K.shape[0] == K.shape[1] == L.shape[0] == L.shape[1]\n",
    "    return K, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_est(K, L):\n",
    "    K, L = _as_kernel_matrices(K, L)\n",
    "    \n",
    "    # TODO: implement the HSIC estimator\n",
    "    \n",
    "    if False:  # the slower way:                          # SOLUTION\n",
    "        n = K.shape[0]                                    # SOLUTION\n",
    "        H = torch.eye(n, dtype=K.dtype, device=K.device)  # SOLUTION\n",
    "        hsic = (H @ K @ H @ L).trace() / (n * n)          # SOLUTION\n",
    "    else:                                                 # SOLUTION\n",
    "        # (I - 1/n 1 1^T) K (I - 1/n 1 1^T)               # SOLUTION\n",
    "        #   = K - 1/n 1 (1^T K) - 1/n (K 1) 1^T           # SOLUTION\n",
    "        #       + 1/n^2 1 (1^T K 1) 1^T                   # SOLUTION\n",
    "        row_means = K.mean(0, keepdim=True)               # SOLUTION\n",
    "        col_means = row_means.t()  # K is symmetric       # SOLUTION\n",
    "        grand_mean = row_means.mean(1, keepdim=True)      # SOLUTION\n",
    "        K_cent = K - row_means - col_means + grand_mean   # SOLUTION\n",
    "        L_transp = L  # L is symmetric too                # SOLUTION\n",
    "        hsic = (K_cent * L_transp).mean()                 # SOLUTION\n",
    "    return hsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a test, we'll also need to do permutations. Note that permutations are slightly different from before: we'll need to permute $K$ and $L$ *separately*, so that $\\{(x_i, y_i)\\}$ becomes $\\{(x_i, y_j)\\}$, and any possible dependence between them is broken.\n",
    "\n",
    "Here's some framework for computing permuations for the HSIC estimator. If you want to do it a faster way, go ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_permutations(K, L, num_permutations=1000, progress=True):\n",
    "    K, L = _as_kernel_matrices(K, L)\n",
    "\n",
    "    est = hsic_est(K, L)\n",
    "\n",
    "    range_ = range(num_permutations)\n",
    "    if progress:\n",
    "        range_ = tqdm(range_)\n",
    "    \n",
    "    n = K.shape[0]       # SOLUTION\n",
    "    stats = []\n",
    "    for i in range_:\n",
    "        pass # TODO: permute the entries of K and L and call hsic_est\n",
    "        oK = torch.randperm(n)            # SOLUTION\n",
    "        Kp = K[oK[:, None], oK[None, :]]  # SOLUTION\n",
    "        oL = torch.randperm(n)            # SOLUTION\n",
    "        Lp = L[oL[:, None], oL[None, :]]  # SOLUTION\n",
    "        stats.append(hsic_est(Kp, Lp))    # SOLUTION\n",
    "        # Note: we really only need to shufle one of them.              # SOLUTION\n",
    "        # And centering isn't affected by the order,                    # SOLUTION\n",
    "        # so it'd be a little faster to center beforehand and keep it.  # SOLUTION\n",
    "        \n",
    "    return est, torch.stack(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on some simple data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "def sample_hsic(n, angle, sigma=0.2, offset=1):\n",
    "    n4 = n // 4\n",
    "    N = np.random.randn(n4, 2) * sigma\n",
    "    S = np.random.randn(n4, 2) * sigma\n",
    "    E = np.random.randn(n4, 2) * sigma\n",
    "    W = np.random.randn(n4, 2) * sigma\n",
    "    \n",
    "    N[:,1] += offset\n",
    "    S[:,1] -= offset\n",
    "    W[:,0] -= offset\n",
    "    E[:,0] += offset\n",
    "    \n",
    "    R = np.array([[np.cos(angle), -np.sin(angle)],[np.sin(angle), np.cos(angle)]])\n",
    "    A = R.dot(np.vstack((N, S, W, E)).T).T\n",
    "    \n",
    "    A = A.astype(np.float32)\n",
    "    np.random.shuffle(A)\n",
    "    return A[:, 0], A[:, 1]\n",
    "\n",
    "N = 200\n",
    "np.random.seed(0)\n",
    "X, Y = sample_hsic(n=N, angle=np.pi / 12)\n",
    "np.savez(\"data/hsic.npz\", X=X[:, None], Y=Y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/hsic.npz') as d:\n",
    "    X, Y = as_tensors(d['X'], d['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there is some very simple dependence going on.\n",
    "The first thing a statistician would do is of course to check for correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation between X and Y:\", np.corrcoef(X.squeeze(), Y.squeeze())[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is very low.\n",
    "A kernel test for independence is more powerful, even with a bandwidth parameter chosen by the median heuristic.\n",
    "\n",
    "**Exercise:** Implement and visualize this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "\n",
    "K = GaussianKernel(X, sigma=median_distance(X))\n",
    "L = GaussianKernel(Y, sigma=median_distance(Y))\n",
    "plot_null_samples(*hsic_permutations(K, L), from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty resounding rejection of the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**: Do we already have the code for doing an actual hypothesis test based on the correlation coefficient? Implement it and run the test to verify that it is not very powerful here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- SOLUTION CELL -->\n",
    "<p style=\"color: blue\">\n",
    "The feature embedding function $\\varphi(x)$ for a linear kernel is just $\\varphi(x) = x$.\n",
    "When we take the product kernel of two of these,\n",
    "we get the outer product,\n",
    "$\\varphi((x, y)) = x y^T$.\n",
    "The HSIC is then\n",
    "$$\n",
    "     \\MMD(\\PP_{XY}, \\PP_X \\times \\PP_Y)^2\n",
    "   = \\left\\lVert \\E [X Y^T] - [\\E X] [\\E Y]^T \\right\\rVert^2 \n",
    ",$$\n",
    "exactly the squared covariance.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "plot_null_samples(*hsic_permutations(LinearKernel(X), LinearKernel(Y)), from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## European parliament documents translations and string kernels\n",
    "\n",
    "We will now do a slightly more elaborate test, that involves some very mild NLP:\n",
    "we'll analyze dependence between documents.\n",
    "We will use transcripts of the Canadian parliament's house debates, downloaded from [here](https://www.isi.edu/natural-language/download/hansard/).\n",
    "Those consist of pairs of French and English transcripts. Here's the end of two corresponding documents:\n",
    "\n",
    "> d until tomorrow at 2 p.m., pursuant to Standing Order 24(1).  \n",
    "> (The House adjourned at 6.41 p.m.) \n",
    "\n",
    "> main, \u00e0 10 heures, conform\u00e9ment \u00e0 l'article 24(1) du R\u00e8glement.  \n",
    "> (La s\u00e9ance est lev\u00e9e \u00e0 18 h 20.) \n",
    "\n",
    "Our question here is whether we can detect this supposedly strong dependence structure using the kernel HSIC.\n",
    "Note that this approach does not rely on attempting to translate the documents, but rather on comparing within-document structure. \n",
    "HSIC compares the self-similarity within the English documents with self-similarity of the French ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to construct a string kernel, a  \"bag of words\" kernel between documents $s$ and $t$,\n",
    "$$\n",
    "k(s,t) = \\phi(s)^\\top \\phi(t)\n",
    "$$\n",
    "where $\\mathcal W$ is the set of all words that we consider,\n",
    "and each element of $\\phi(x)\\in\\mathbb{N}^{|\\mathcal{W}|}$ contains the number of times that a particular word $w\\in\\mathcal{W}$ appears in $x$.\n",
    "The kernel value will be larger if a word appears in both documents many times.\n",
    "We're going to keep things simple and actually construct the representation $\\phi$ for each document.\n",
    "\n",
    "There are also better choices for string kernels, that actually take into account the structure of a word.\n",
    "(Shogun has efficient implementations of many classical ones, usually based on low-level dynamic programming concepts;\n",
    "you can also define them based on modern word embeddings.)\n",
    "But simple word counts will be enough for us here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords, from nltk.corpus.stopwords:\n",
    "# words like \"the\", \"our\" (in English) or \"le\", \"avec\" (in French)\n",
    "# that we don't want overwhelming the meaningful words, so we just skip\n",
    "with open('data/stopwords-english.txt') as f:\n",
    "    en_stop = {x.strip() for x in f}\n",
    "with open('data/stopwords-french.txt') as f:\n",
    "    fr_stop = {x.strip() for x in f}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $\\phi$, we would normally need to do a first pass to explicitly keep track of all the words $\\mathcal W$. This can be kind of slow, though, so instead we're going to use the \"hashing trick.\" Rather than keeping track of a vocabulary, we use a hash function ([MurmurHash](https://en.wikipedia.org/wiki/MurmurHash)) to map each word to a (probably) unique index. To avoid hash collisions, we make $\\phi$ very high-dimensional ($2^{20} \\approx 1,000,000$), but use a sparse representation so that doesn't take much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tarfile\n",
    "from scipy import sparse\n",
    "\n",
    "en_feats = {}\n",
    "fr_feats = {}\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "en_vec = HashingVectorizer(\n",
    "    analyzer=\"word\", decode_error='replace', stop_words=en_stop,\n",
    ")\n",
    "fr_vec = HashingVectorizer(\n",
    "    analyzer=\"word\", decode_error='replace', stop_words=fr_stop,\n",
    ")\n",
    "\n",
    "with tarfile.open('data/transcripts.tar.bz2') as tar:\n",
    "    for info in tqdm(tar, total=627):\n",
    "        fname = info.name\n",
    "        is_french = fname.endswith(\"f\")\n",
    "        is_english = fname.endswith(\"e\")\n",
    "        if not (is_english or is_french):\n",
    "            continue\n",
    "        \n",
    "        with tar.extractfile(info) as f:\n",
    "            if is_english:\n",
    "                en_feats[fname] = en_vec.transform([f.read()])\n",
    "            else:\n",
    "                fr_feats[fname] = fr_vec.transform([f.read()])\n",
    "\n",
    "assert len(en_feats) == len(fr_feats)\n",
    "names = {k[:-1] for k in en_feats}\n",
    "rrelation coefficient\n",
    "X = sparse.vstack([en_feats[k + 'e'] for k in names])\n",
    "Y = sparse.vstack([fr_feats[k + 'f'] for k in names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features are enormous (and enormously sparse) matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Construct linear kernel matrices for the French and English data.\n",
    "\n",
    "Now, our PyTorch kernel infrastructure isn't going to love trying to construct a matrix with a million columns. We're only going to use a linear kernel, though, so we can just compute the kernel ourselves. (You can convert from these sparse matrix classes to numpy arrays with `toarray()`, but do it *after* computing the kernel matrix to avoid trying to allocate an enormous dense array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute kernels\n",
    "K_XX = (\n",
    "    # ...\n",
    "    X @ X.T   # SOLUTION\n",
    ").toarray().astype(np.float32)\n",
    "K_YY = (\n",
    "    # ...\n",
    "    Y @ Y.T   # SOLUTION\n",
    ").toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Run the HSIC test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "plot_null_samples(*hsic_permutations(K_XX, K_YY), from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the English and French documents are not independent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlss-testing]",
   "language": "python",
   "name": "conda-env-mlss-testing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
